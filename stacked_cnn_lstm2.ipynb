{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Sentiment-Analysis-Deep-Learning-Methods-For-Speech-Recognition/blob/main/stacked_cnn_lstm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHANGE TO GPU + CHECK GOOGLE PRO BEFORE RUNNING!**"
      ],
      "metadata": {
        "id": "wYL2-X_mQJZF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbRkAw1RVNbn"
      },
      "source": [
        "# Load file names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZoPfL8cKyzv",
        "outputId": "c80e3c36-da26-4ff5-dcba-59e5cb6bd71b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "20SnQEifVNbq"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "\n",
        "\n",
        "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
        "DATA_PATH = '/content/drive/MyDrive/audio_speech_actors_01-24'\n",
        "SAMPLE_RATE = 48000\n",
        "\n",
        "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
        "for dirname, _, filenames in os.walk(DATA_PATH):\n",
        "    for filename in filenames:\n",
        "        file_path = os.path.join('/',dirname, filename)\n",
        "        identifiers = filename.split('.')[0].split('-')\n",
        "        emotion = (int(identifiers[2]))\n",
        "        if emotion == 8: # promeni surprise sa 8 na 0\n",
        "            emotion = 0\n",
        "        if int(identifiers[3]) == 1:\n",
        "            emotion_intensity = 'normal' \n",
        "        else:\n",
        "            emotion_intensity = 'strong'\n",
        "        if int(identifiers[6])%2 == 0:\n",
        "            gender = 'female'\n",
        "        else:\n",
        "            gender = 'male'\n",
        "        \n",
        "        data = data.append({\"Emotion\": emotion,\n",
        "                            \"Emotion intensity\": emotion_intensity,\n",
        "                            \"Gender\": gender,\n",
        "                            \"Path\": file_path\n",
        "                             },\n",
        "                             ignore_index = True\n",
        "                          )\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "Sf5D86-hVNbs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "3c3a0731-b651-4c0d-910b-7981c6117ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of files is 1440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Emotion Emotion intensity Gender  \\\n",
              "0       1            normal   male   \n",
              "1       1            normal   male   \n",
              "2       1            normal   male   \n",
              "3       1            normal   male   \n",
              "4       2            normal   male   \n",
              "\n",
              "                                                Path  \n",
              "0  /content/drive/MyDrive/audio_speech_actors_01-...  \n",
              "1  /content/drive/MyDrive/audio_speech_actors_01-...  \n",
              "2  /content/drive/MyDrive/audio_speech_actors_01-...  \n",
              "3  /content/drive/MyDrive/audio_speech_actors_01-...  \n",
              "4  /content/drive/MyDrive/audio_speech_actors_01-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1c4ed68-fdea-4336-b7d2-aff9aaa87def\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion intensity</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/content/drive/MyDrive/audio_speech_actors_01-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/content/drive/MyDrive/audio_speech_actors_01-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/content/drive/MyDrive/audio_speech_actors_01-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/content/drive/MyDrive/audio_speech_actors_01-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/content/drive/MyDrive/audio_speech_actors_01-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1c4ed68-fdea-4336-b7d2-aff9aaa87def')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1c4ed68-fdea-4336-b7d2-aff9aaa87def button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1c4ed68-fdea-4336-b7d2-aff9aaa87def');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "print(\"number of files is {}\".format(len(data)))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwW1ZFlcVNbs"
      },
      "source": [
        "number of examples per Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ERBf93UVNbt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "079b0fce-d3b9-4270-c089-a16db2d58f1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of examples')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEJCAYAAAB7UTvrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc2ElEQVR4nO3de5gdVZnv8e+PAILcAqSHkwFiAyeDD+oxSg8DikwAcQDlIoqScbiNGjkHRGb0jIgIjMqIInpEZsBw4HARuYlcBEQgysWZg9KBQBIgcgsDGJKIQALILXnnj7W6stPZ3V29e9euNv37PM9+umpV1ap3V+3d7151WaWIwMzMDGCtugMwM7PRw0nBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMysUFlSkLS1pF9KekDSPEmfy+WbSbpF0sP576a5XJLOlPSIpPslvbuq2MzMrLkqWwpvAJ+PiB2AnYGjJe0AHA/MjIjJwMw8DrAPMDm/pgNnVxibmZk1sXZVFUfEQmBhHl4m6UFgS+AAYGqe7ULgNuCLufyiSHfT3SVpvKSJuZ6mJkyYEN3d3VW9BTOzNdKsWbN+HxFdzaZVlhQaSeoG3gX8Gtii4R/9M8AWeXhL4MmGxZ7KZQMmhe7ubnp7e9sdrpnZGk3SEwNNq/xEs6QNgauA4yJiaeO03CoYVj8bkqZL6pXUu2TJkjZGamZmlSYFSeuQEsIlEfGTXLxI0sQ8fSKwOJc/DWzdsPhWuWwVETEjInoioqerq2nrx8zMWlTl1UcCzgMejIjvNEy6Djg8Dx8OXNtQfli+Cmln4IXBzieYmVn7VXlO4b3AocAcSbNz2QnAacAVkj4JPAF8LE+7EdgXeAR4GTiywtjMzKyJKq8++hWgASbv2WT+AI6uKh4zMxua72g2M7OCk4KZmRWcFMzMrOCkYGZmhY7c0TxadR9/Q23rXnDaBwed7tiac2ytcWytGSq2NZFbCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmaFypKCpPMlLZY0t6Hsckmz82tB37ObJXVL+mPDtHOqisvMzAZWZdfZFwBnARf1FUTEx/uGJZ0BvNAw/6MRMaXCeMzMbAiVJYWIuENSd7NpkgR8DNijqvWbmdnw1XVO4X3Aooh4uKFsG0n3Srpd0vtqisvMbEyr68lr04BLG8YXApMi4llJOwLXSHpbRCztv6Ck6cB0gEmTJnUkWDOzsaLjLQVJawMHAZf3lUXEqxHxbB6eBTwK/EWz5SNiRkT0RERPV1dXJ0I2Mxsz6jh89H7goYh4qq9AUpekcXl4W2Ay8FgNsZmZjWlVXpJ6KfD/ge0lPSXpk3nSIax66AhgN+D+fInqj4GjIuIPVcVmZmbNVXn10bQByo9oUnYVcFVVsZiZWTm+o9nMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWqPIZzedLWixpbkPZKZKeljQ7v/ZtmPYlSY9Imi/pb6qKy8zMBlZlS+ECYO8m5d+NiCn5dSOApB2AQ4C35WX+TdK4CmMzM7MmKksKEXEH8IeSsx8AXBYRr0bE48AjwE5VxWZmZs3VcU7hGEn358NLm+ayLYEnG+Z5KpeZmVkHdTopnA1sB0wBFgJnDLcCSdMl9UrqXbJkSbvjMzMb0zqaFCJiUUQsj4gVwLmsPET0NLB1w6xb5bJmdcyIiJ6I6Onq6qo2YDOzMWbIpCBpO0lvysNTJR0raXwrK5M0sWH0w0DflUnXAYdIepOkbYDJwG9aWYeZmbVu7RLzXAX0SPrvwAzgWuBHwL6DLSTpUmAqMEHSU8DJwFRJU4AAFgCfAYiIeZKuAB4A3gCOjojlrbwhMzNrXZmksCIi3pD0YeD7EfF9SfcOtVBETGtSfN4g858KnFoiHjMzq0iZcwqvS5oGHA5cn8vWqS4kMzOrS5mkcCSwC3BqRDyej/lfXG1YZmZWhyEPH0XEA5K+CEzK448D36w6MDMz67wyVx/tB8wGbsrjUyRdV3VgZmbWeWUOH51Cup/geYCImA1sW2FMZmZWk1InmiPihX5lK6oIxszM6lXmktR5kv4WGCdpMnAs8B/VhmVmZnUo01L4LKlL61eBS4GlwHFVBmVmZvUoc/XRy8CX88vMzNZgAyYFST8ldUfRVETsX0lEZmZWm8FaCt/uWBRmZjYqDJgUIuL2vmFJ6wJvJbUc5kfEax2IzczMOmzIcwqSPgicAzwKCNhG0mci4mdVB2dmZp1V5pLUM4DdI+IRSM9XAG4AnBTMzNYwZS5JXdaXELLHgGUVxWNmZjUq01LolXQjcAXpnMLBwN2SDgKIiJ9UGJ+ZmXVQmaSwHrAI+Os8vgRYH9iPlCScFMzM1hBlbl47shOBmJlZ/cpcfbQNqauL7sb5h7p5TdL5wIeAxRHx9lx2OqmF8RrpaqYjI+J5Sd3Ag8D8vPhdEXHUMN+LmZmNUJnDR9eQnq38U4bXO+oFwFnARQ1ltwBfys98/ibwJeCLedqjETFlGPWbmVmblUkKr0TEmcOtOCLuyC2AxrKbG0bvAj463HrNzKw6ZZLC9ySdDNxM6ikVgIi4Z4Tr/nvg8obxbSTdS+qF9cSIuHOE9ZuZ2TCVSQrvAA4F9mDl4aPI4y2R9GXgDeCSXLQQmBQRz0raEbhG0tsiYmmTZacD0wEmTZrUaghmZtZEmaRwMLBtu/o7knQE6QT0nhERABHxKrkVEhGzJD0K/AXQ23/5iJgBzADo6ekZsBdXMzMbvjJ3NM8FxrdjZZL2Bv4J2D8/p6GvvEvSuDy8LTCZdOe0mZl1UJmWwnjgIUl3s+o5haEuSb0UmApMkPQUcDLpaqM3AbdIgpWXnu4GfFXS66RDVEdFxB+G/3bMzGwkyiSFk1upOCKmNSk+b4B5rwKuamU9ZmbWPmXuaL59qHnMzGzNMOQ5BUk7S7pb0ouSXpO0XNJqVwWZmdmfvjInms8CpgEPkzrC+xTwr1UGZWZm9SiTFMjPUxgXEcsj4v8Be1cblpmZ1aHMieaX8zOaZ0v6FulGs1LJxMzM/rSU+ed+aJ7vGOAlYGvgI1UGZWZm9SjTUvhjRLwCvAL8M4Ck7SuNyszMalGmpXCnpI/1jUj6PHB1dSGZmVldyrQUpgIzJB0MbEF6GM5OVQZlZmb1GLKlEBELgZuAXUhPX7swIl6sOC4zM6tBmcdx3gr8Dng76STzeZLuiIgvVB2cmZl1Vqmb1yLisIh4PiLmkFoML1Qcl5mZ1aDM4aNrJO0q6chctCnww2rDMjOzOpTp++hk4Iukbq8B1sVJwcxsjVTm8NGHgf1JN64REb8DNqoyKDMzq0eZpPBafmxmAEjaoNqQzMysLmWSwhWSfgCMl/Rp4Fbg3GrDMjOzOpR5yM63Je0FLAW2B06KiFsqj8zMzDquzB3N5CQw7EQg6XzgQ8DiiHh7LtsMuJx0I9wC4GMR8ZzSQ5u/B+wLvAwcERH3DHedZmbWuqq7wL6A1Z+9cDwwMyImAzPzOMA+wOT8mg6cXXFsZmbWT6VJISLuAP7Qr/gA4MI8fCFwYEP5RZHcRTqHMbHK+MzMbFUDJgVJM/Pfb7Z5nVvk/pQAniF1sgewJfBkw3xP5TIzM+uQwc4pTJT0HmB/SZcBapzYjuP9ERGSYjjLSJpOOrzEpEmTRhqCmZk1GCwpnAR8BdgK+E6/aQHs0eI6F0maGBEL8+Ghxbn8aVKHe322ymWrrjhiBjADoKenZ1gJxczMBjfg4aOI+HFE7AN8KyJ27/dqNSEAXAccnocPB65tKD9Myc7ACw2HmczMrAPK3KfwNUn7A7vlotsi4voylUu6lPSQngmSngJOBk4j3RD3SeAJoO+pbjeSLkd9hHRJ6pGrVWhmZpUq8zyFb5CetHZJLvqcpPdExAlDLRsR0waYtGeTeQM4eqg6zcxGi+7jb6ht3QtO+2Al9Za5ee2DwJSIWAEg6ULgXmDIpGBmZn9ayt6nML5heJMqAjEzs/qVaSl8A7hX0i9Jl6Xuxsq7kM3MbA1S5kTzpZJuA/4yF30xIp6pNCozM6tF2Q7xFpIuGTUzszVY1R3imZnZnxAnBTMzKwyaFCSNk/RQp4IxM7N6DZoUImI5MF+Se54zMxsDypxo3hSYJ+k3wEt9hRGxf2VRmZlZLcokha9UHoWZmY0KZe5TuF3SW4DJEXGrpDcD46oPzczMOm3Iq48kfRr4MfCDXLQlcE2VQZmZWT3KXJJ6NPBeYClARDwM/FmVQZmZWT3KJIVXI+K1vhFJa5OevGZmZmuYMknhdkknAOtL2gu4EvhptWGZmVkdyiSF44ElwBzgM6QnpJ1YZVBmZlaPMlcfrcgP1vk16bDR/PyUNDMzW8OUeRznB4FzgEdJz1PYRtJnIuJnraxQ0vbA5Q1F2wInkR7k82lSqwTghIi4sZV1mJlZa8rcvHYGsHtEPAIgaTvgBqClpBAR84Epua5xwNPA1cCRwHcj4tut1GtmZiNX5pzCsr6EkD0GLGvT+vcEHo2IJ9pUn5mZjcCALQVJB+XBXkk3AleQzikcDNzdpvUfAlzaMH6MpMOAXuDzEfFcm9ZjZmYlDNZS2C+/1gMWAX8NTCUd819/pCuWtC6wP+kSV4Czge1Ih5YWkg5bNVtuuqReSb1LlixpNouZmbVowJZCRBxZ8br3Ae6JiEV5fYv6Jkg6F7h+gLhmADMAenp6fBWUmVkblbn6aBvgs0B34/xt6Dp7Gg2HjiRNzM+CBvgwMHeE9ZuZ2TCVufroGuA80l3MK9qxUkkbAHuRbobr8y1JU0jnLRb0m2ZmZh1QJim8EhFntnOlEfESsHm/skPbuQ4zMxu+Mknhe5JOBm4GXu0rjIh7KovKzMxqUSYpvAM4FNiDlYePIo+bmdkapExSOBjYtrH7bDMzWzOVuaN5LqlfIjMzW8OVaSmMBx6SdDernlMY6SWpZmY2ypRJCidXHoWZmY0KZZ6ncHsnAjEzs/qVuaN5GSufybwusA7wUkRsXGVgZmbWeWVaChv1DUsScACwc5VBmZlZPcpcfVSI5BrgbyqKx8zMalTm8NFBDaNrAT3AK5VFZGZmtSlz9dF+DcNvkDqrO6CSaMzMrFZlzilU/VwFMzMbJQZ7HOdJgywXEfG1CuIxM7MaDdZSeKlJ2QbAJ0ndXjspmJmtYQZ7HGfxjGRJGwGfA44ELmOA5yebmdmftkHPKUjaDPhH4BPAhcC7I+K5TgRmZmadN9g5hdOBg4AZwDsi4sWORWVmZrUY7Oa1zwN/DpwI/E7S0vxaJmnpSFcsaYGkOZJmS+rNZZtJukXSw/nvpiNdj5mZlTdgUoiItSJi/YjYKCI2bnht1MZ+j3aPiCkR0ZPHjwdmRsRkYGYeNzOzDhlWNxcdcADp3AX574E1xmJmNubUmRQCuFnSLEnTc9kWEbEwDz8DbFFPaGZmY1OZbi6qsmtEPC3pz4BbJD3UODEiQlL0XygnkOkAkyZN6kykZmZjRG0thYh4Ov9dDFwN7AQskjQRIP9d3GS5GRHRExE9XV1dnQzZzGyNV0tSkLRBviEOSRsAHwDmAtcBh+fZDgeurSM+M7Oxqq7DR1sAV6dn9rA28KOIuEnS3cAVkj4JPAF8rKb4zMzGpFqSQkQ8BryzSfmzwJ6dj8jMzGD0XZJqZmY1clIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs0LHk4KkrSX9UtIDkuZJ+lwuP0XS05Jm59e+nY7NzGysq+MZzW8An4+IeyRtBMySdEue9t2I+HYNMZmZGTUkhYhYCCzMw8skPQhs2ek4zMxsdbWeU5DUDbwL+HUuOkbS/ZLOl7RpbYGZmY1RtSUFSRsCVwHHRcRS4GxgO2AKqSVxxgDLTZfUK6l3yZIlHYvXzGwsqCUpSFqHlBAuiYifAETEoohYHhErgHOBnZotGxEzIqInInq6uro6F7SZ2RhQx9VHAs4DHoyI7zSUT2yY7cPA3E7HZmY21tVx9dF7gUOBOZJm57ITgGmSpgABLAA+U0NsZmZjWh1XH/0KUJNJN3Y6FjMzW5XvaDYzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVRl1SkLS3pPmSHpF0fN3xmJmNJaMqKUgaB/wrsA+wAzBN0g71RmVmNnaMqqQA7AQ8EhGPRcRrwGXAATXHZGY2Zoy2pLAl8GTD+FO5zMzMOkARUXcMBUkfBfaOiE/l8UOBv4qIYxrmmQ5Mz6PbA/M7HmgyAfh9TeseimNrjWNrjWNrTZ2xvSUiuppNWLvTkQzhaWDrhvGtclkhImYAMzoZVDOSeiOip+44mnFsrXFsrXFsrRmtsY22w0d3A5MlbSNpXeAQ4LqaYzIzGzNGVUshIt6QdAzwc2AccH5EzKs5LDOzMWNUJQWAiLgRuLHuOEqo/RDWIBxbaxxbaxxba0ZlbKPqRLOZmdVrtJ1TMDOzGjkpVEDSjZLG1xxDt6S/bXHZF9sdT0PdR0g6q6r68zq6Jc2tch2jxVh6r81IOlbSg5IuqTmOUyR9QdJXJb2/A+s7sKreHpwUSpBU6tyLkrUiYt+IeL7quIbQDTRNCmXfj9lIdeCz9r+AvSLiE61W0M4YI+KkiLi1XfUN4kBSV0BtN6aSgqQNJN0g6T5JcyV9XNICSRPy9B5Jt+XhUyRdLOnfgYvzL9xrJd0m6WFJJ+f5unMHfhcBc4Gt++pstr68zI6Sbpc0S9LPJU1siLE7//I5V9I8STdLWl/SdpJuysvcKemtef4L8k1/fcv3/co/DXifpNmS/iHHf52kXwAzJW0oaaakeyTNkTSi7kQkHSbp/vxeL5a0n6RfS7pX0q2StmiyzAWSzpZ0l6THJE2VdH5+/xeMJB5gXJNt+GlJd+cYr5L05oY4zpHUK+m3kj6Uywfa51+VdFzD+zhV0udGEuwAn82TcrxzJc2QpDzvjnm++4CjR7LeJnFckz9j85RuFEXSi/k93pf31Ra5fLs8PkfS1/s+e3k/3inpOuCBKrZXruccYFvgZ5K+nD87v8mfuQPyPN05lnvy6z3NYmxx/V/On5dfkW6kXeX7KOk0SQ/k78W3c9lg2+z6hrrPknREs3rye9gfOD1/v7draQMOJCLGzAv4CHBuw/gmwAJgQh7vAW7Lw6cAs4D18/gRwEJgc2B9UgLoIf0iXwHs3FDvAtLdis3Wtw7wH0BXLvs46dLbvnm6gTeAKXn8CuDvgJnA5Fz2V8Av8vAFwEcbln8x/50KXN9QfgSp25DN8vjawMZ5eALwCCsvPHhxmNv1bcBvG7bjZsCmDfV9CjijIY6zGmK/DBCpj6ulwDtIP1Zm9W2DFvbzQNtw84Z5vg58tiGOm/J6J+fttN4Q+/yevOxawKONdbfxs7lZw/jFwH55+H5gtzx8OjC3jd+Rvs9H3/vdHIiGdX8LODEPXw9My8NH9fvsvQRs07A/2rq9mnzX/gX4u1w2Pn8eNwDeDKyXyycDvc1ibGG9OwJzcv0bk74/X8ifpY/m7Tafld+B8SW2WeP39az8+Ruongto+N638zWmWgqknbiXpG9Kel9EvDDE/NdFxB8bxm+JiGdz2U+AXXP5ExFxV8n1bQ+8HbhF0mzgRNKd240ej4jZeXgW6Uv1HuDKvMwPgIkM3y0R8Yc8LOBfJN0P3ErqY2q1X/Ml7QFcGRG/B8jr2Ar4uaQ5wP8mJY5mfhrpUz4HWBQRcyJiBTCP9L5b1Wwbvj3/OpwDfKJfTFdExIqIeBh4DHhrLl9tn0fEAuBZSe8CPgDcGxHPjiBWaP5Z2T23tuaQtvHblM5VjY+IO/JyF49wvf0dm1sgd5F6F5gMvEb6ZwYrtyXALsCVefhH/er5TUQ8DlDR9urvA8Dx+ftxGympTyL9CDs3b8MrWfWQSxFjC94HXB0RL0fEUla/yfYF4BXgPEkHAS/n8sG2WTMD1VOZMXVsOSJ+K+ndwL7A1yXNJP2i7EuO6/Vb5KX+VQww3n++wdZ3NTAvInYZJNRXG4aXk/5ZPx8RU5rMW8QvaS1g3UHqbYzzE0AXsGNEvC5pAau//5H4PvCdiLhO0lRSy6uZvve6glXf9wpG9vnsvw3XJ/26OjAi7stN86kN8wy0bwcq/7+kX3L/DTh/BHGmSpt/Vo4GeiLiSUmn0N79s5q8n94P7BIRLysdSl0PeD0nbkjbssx+6f+daOv2akLARyJilb7Q8nZbBLyT9D15ZZAY2ybSjbg7AXuSWg7HkBL7QBr/D0He1y3UM2JjqqUg6c+BlyPih6Rm97tJzc8d8ywfGaKKvSRtJml90omef29hffOBLkm75HnWkTTQr+g+S4HHJR2cl5Gkd+ZpjfHvT/plBLAM2GiQOjcBFueEsDvwliFiGMwvgIMlbZ7j2yzX39dv1eEjqLudNgIWSlqHlBQbHSxprXx8dltWdrQ40D6/Gtgb+EvSHfgjMsBnBeD3kjYk/UMg0gUMz0vqa6W2fIK1iU2A53JCeCuw8xDz38XK78whQ8zb1u3VxM+Bz0rFeZd35fJNgIW59XkoqaeEdrgDOFDpXNVGwH6NE/M+2yTSzbj/QEpKMPA2ewLYQdKbcmtwzyHqGer73bIx1VIgHa8+XdIK4HXgf5J+QZ4n6WukZudgfgNcRTo08sOI6JXUPZz1RcRr+UTUmZI2Ie2D/0M6XDKYTwBnSzqR9I//MuA+4Fzg2tzkv4mVv37uB5bn8guA5/rVdwnw09ys7gUeGmL9A4qIeZJOBW6XtBy4l9QyuFLSc6SksU2r9bfRV4BfA0vy38Yv1X+S9u/GwFER8Ur+/7LaPgfI+/GXpBbc8jbE1uyzeSDpuP4zpH7B+hwJnC8pgJvbsO4+NwFHSXqQlBSbHRJtdBzwQ0lfzssOeDi2gu3V39dI36P7c4v5ceBDwL8BV0k6jFW/HyMSEfdIupz0HVzMqvsH0mfrWknrkVox/5jLm26z3Bq8grS/Hyd9hwar5zLSYbFjSecWHm3H+wLf0VxaPtzQEw3deNuaQelKp+sj4sf9yo9ggH2e//HcAxycz0OMOUpXb/0xIkLSIaQTqE2vYvP2Soazzeoy1loKZiOmdNPQ9aQTjWP2HxzpsOVZ+ZDN88DfN5vJ22sVpbZZndxSMDOzwpg60WxmZoNzUjAzs4KTgpmZFZwUzDJJy3NfMn2v49tQ5yq91Sr1r3XmSOs1q4pPNJtlkl6MiA3bXOdU4AsR8aF21mtWFbcUzIag1OvtN3LroVfSu5V6t31U0lF5Hkk6XalH0znKPeKyem+1RW+Y+U7pa5R6v7xL0v/I5aco9fh5m1LvscfW885tLPJ9CmYrra/UoVqfb0TE5Xn4PyNiiqTvku4Qfy+pf5q5wDnAQcAUUjcEE4C7Jd0BHE9DSyG3HPr8M6lzuAMl7QFclOuA1CHf7qQ7WudLOjsiXm/3Gzbrz0nBbKU/DtDpIKzsBXMOsGFELAOWSXo191WzK3Bp7sJhkaTbSf38LB1kfbuS+8GJiF9I2lzSxnnaDRHxKvCqpMWkThGfGtG7MyvBh4/MyqmqN9eh1gfleyY1GzEnBbP2uBP4uKRxkrqA3Uid6Q3Wm+Wd5F5O82Gl3+e++c1q418fZiv1P6dwU0SUvSz1atIDVO4jPXPhnyLiGUnPsmpvtfc2LHMKqbfT+0kPTxktXYzbGOZLUs3MrODDR2ZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwK/wWecoQVwK412gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
        "ax.set_xticks(ticks=range(8))\n",
        "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
        "ax.set_xlabel('Emotion')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vKaQhFmVNbu"
      },
      "source": [
        "number of examples per gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJFLgtVeVNbu"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "counts = data['Gender'].value_counts()\n",
        "ax.bar(x=[0,1], height=counts.values)\n",
        "ax.set_xticks(ticks=[0,1])\n",
        "ax.set_xticklabels(list(counts.index))\n",
        "ax.set_xlabel('Gender')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-7T0x-VNbv"
      },
      "source": [
        "number of examples per emotion intensity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7MdEBwgVNbv"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "counts = data['Emotion intensity'].value_counts()\n",
        "ax.bar(x=[0,1], height=counts.values)\n",
        "ax.set_xticks(ticks=[0,1])\n",
        "ax.set_xticklabels(list(counts.index))\n",
        "ax.set_xlabel('Gender')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTzEdCkVNbw"
      },
      "source": [
        "# Load the signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K6k60yuVNbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef57a979-de07-49cd-f607-11208ad0e416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 413/1440 files"
          ]
        }
      ],
      "source": [
        "mel_spectrograms = []\n",
        "signals = []\n",
        "for i, file_path in enumerate(data.Path):\n",
        "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
        "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
        "    signal[:len(audio)] = audio\n",
        "    signals.append(signal)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
        "signals = np.stack(signals,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQqGSjQLVNbx"
      },
      "source": [
        "# Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3uzoW2cVNbx"
      },
      "outputs": [],
      "source": [
        "X = signals\n",
        "train_ind,test_ind,val_ind = [],[],[]\n",
        "X_train,X_val,X_test = [],[],[]\n",
        "Y_train,Y_val,Y_test = [],[],[]\n",
        "for emotion in range(len(EMOTIONS)):\n",
        "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
        "    emotion_ind = np.random.permutation(emotion_ind)\n",
        "    m = len(emotion_ind)\n",
        "    ind_train = emotion_ind[:int(0.8*m)]\n",
        "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
        "    ind_test = emotion_ind[int(0.9*m):]\n",
        "    X_train.append(X[ind_train,:])\n",
        "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
        "    X_val.append(X[ind_val,:])\n",
        "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
        "    X_test.append(X[ind_test,:])\n",
        "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
        "    train_ind.append(ind_train)\n",
        "    test_ind.append(ind_test)\n",
        "    val_ind.append(ind_val)\n",
        "X_train = np.concatenate(X_train,0)\n",
        "X_val = np.concatenate(X_val,0)\n",
        "X_test = np.concatenate(X_test,0)\n",
        "Y_train = np.concatenate(Y_train,0)\n",
        "Y_val = np.concatenate(Y_val,0)\n",
        "Y_test = np.concatenate(Y_test,0)\n",
        "train_ind = np.concatenate(train_ind,0)\n",
        "val_ind = np.concatenate(val_ind,0)\n",
        "test_ind = np.concatenate(test_ind,0)\n",
        "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
        "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
        "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
        "# check if all are unique\n",
        "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
        "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
        "\n",
        "del X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoAJ6nZzVNbx"
      },
      "source": [
        "# Augment signals by adding AWGN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZBJy6P5VNby"
      },
      "outputs": [],
      "source": [
        "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
        "    signal_len = len(signal)\n",
        "    # Generate White Gaussian noise\n",
        "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
        "    # Normalize signal and noise\n",
        "    norm_constant = 2.0**(num_bits-1)\n",
        "    signal_norm = signal / norm_constant\n",
        "    noise_norm = noise / norm_constant\n",
        "    # Compute signal and noise power\n",
        "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
        "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
        "    # Random SNR: Uniform [15, 30] in dB\n",
        "    target_snr = np.random.randint(snr_low, snr_high)\n",
        "    # Compute K (covariance matrix) for each noise \n",
        "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
        "    K = np.ones((signal_len, augmented_num)) * K  \n",
        "    # Generate noisy signal\n",
        "    return signal + K.T * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgwTVlxJVNby"
      },
      "outputs": [],
      "source": [
        "aug_signals = []\n",
        "aug_labels = []\n",
        "for i in range(X_train.shape[0]):\n",
        "    signal = X_train[i,:]\n",
        "    augmented_signals = addAWGN(signal)\n",
        "    for j in range(augmented_signals.shape[0]):\n",
        "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
        "        aug_signals.append(augmented_signals[j,:])\n",
        "        data = data.append(data.iloc[i], ignore_index=True)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
        "aug_signals = np.stack(aug_signals,axis=0)\n",
        "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
        "aug_labels = np.stack(aug_labels,axis=0)\n",
        "Y_train = np.concatenate([Y_train,aug_labels])\n",
        "print('')\n",
        "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrZXT5hVNbz"
      },
      "source": [
        "# Calculate mel spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiWEtIfuVNb0"
      },
      "outputs": [],
      "source": [
        "def getMELspectrogram(audio, sample_rate):\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
        "                                              sr=sample_rate,\n",
        "                                              n_fft=1024,\n",
        "                                              win_length = 512,\n",
        "                                              window='hamming',\n",
        "                                              hop_length = 256,\n",
        "                                              n_mels=128,\n",
        "                                              fmax=sample_rate/2\n",
        "                                             )\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return mel_spec_db\n",
        "\n",
        "# test function\n",
        "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
        "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
        "signal[:len(audio)] = audio\n",
        "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
        "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
        "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uekXrF7BVNb0"
      },
      "outputs": [],
      "source": [
        "mel_train = []\n",
        "print(\"Calculatin mel spectrograms for train set\")\n",
        "for i in range(X_train.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_train.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
        "print('')\n",
        "del X_train\n",
        "\n",
        "mel_val = []\n",
        "print(\"Calculatin mel spectrograms for val set\")\n",
        "for i in range(X_val.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_val.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
        "print('')\n",
        "del X_val\n",
        "\n",
        "mel_test = []\n",
        "print(\"Calculatin mel spectrograms for test set\")\n",
        "for i in range(X_test.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_test.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
        "print('')\n",
        "del X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNXWko_hVNb1"
      },
      "source": [
        "# Split into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKl22PFbVNb1"
      },
      "outputs": [],
      "source": [
        "def splitIntoChunks(mel_spec,win_size,stride):\n",
        "    t = mel_spec.shape[1]\n",
        "    num_of_chunks = int(t/stride)\n",
        "    chunks = []\n",
        "    for i in range(num_of_chunks):\n",
        "        chunk = mel_spec[:,i*stride:i*stride+win_size]\n",
        "        if chunk.shape[1] == win_size:\n",
        "            chunks.append(chunk)\n",
        "    return np.stack(chunks,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OADpVxkBVNb1"
      },
      "outputs": [],
      "source": [
        "# get chunks\n",
        "# train set\n",
        "mel_train_chunked = []\n",
        "for mel_spec in mel_train:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_train_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
        "# val set\n",
        "mel_val_chunked = []\n",
        "for mel_spec in mel_val:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_val_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
        "# test set\n",
        "mel_test_chunked = []\n",
        "for mel_spec in mel_test:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_test_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzQy-0JBVNb2"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vapt6LfNVNb3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# BATCH FIRST TimeDistributed layer\n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "        # squash samples and timesteps into a single axis\n",
        "        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n",
        "        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n",
        "        else: # (samples,timesteps,inp1,inp2,inp3)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n",
        "            \n",
        "        y = self.module(x_reshape)\n",
        "        \n",
        "        # we have to reshape Y\n",
        "        if len(x.size()) == 3:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n",
        "        elif len(x.size()) == 4:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n",
        "        else:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLG4M32EVNb3"
      },
      "outputs": [],
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self,num_emotions):\n",
        "        super().__init__()\n",
        "        # conv block\n",
        "        self.conv2Dblock = nn.Sequential(\n",
        "            # 1. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=1,\n",
        "                                   out_channels=16,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(16)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 2. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=16,\n",
        "                                   out_channels=32,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(32)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 3. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=32,\n",
        "                                   out_channels=64,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(64)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 4. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=64,\n",
        "                                   out_channels=128,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(128)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4))\n",
        "        )\n",
        "        # LSTM block\n",
        "        hidden_size = 64\n",
        "        self.lstm = nn.LSTM(input_size=128,hidden_size=hidden_size,bidirectional=False, batch_first=True) \n",
        "        self.dropout_lstm = nn.Dropout(p=0.3)\n",
        "        # Linear softmax layer\n",
        "        self.out_linear = nn.Linear(hidden_size,num_emotions)\n",
        "    def forward(self,x):\n",
        "        conv_embedding = self.conv2Dblock(x)\n",
        "        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n",
        "        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n",
        "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
        "        # lstm_embedding (batch, time, hidden_size)\n",
        "        lstm_output = lstm_embedding[:,-1,:] \n",
        "        output_logits = self.out_linear(lstm_output)\n",
        "        output_softmax = nn.functional.softmax(output_logits,dim=1)\n",
        "        return output_logits, output_softmax\n",
        "                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez_7eMZoVNb4"
      },
      "outputs": [],
      "source": [
        "def loss_fnc(predictions, targets):\n",
        "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo1iRY8yVNb5"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsDMnMs3VNb5"
      },
      "outputs": [],
      "source": [
        "def make_train_step(model, loss_fnc, optimizer):\n",
        "    def train_step(X,Y):\n",
        "        # set model to train mode\n",
        "        model.train()\n",
        "        # forward pass\n",
        "        output_logits, output_softmax = model(X)\n",
        "        predictions = torch.argmax(output_softmax,dim=1)\n",
        "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "        # compute loss\n",
        "        loss = loss_fnc(output_logits, Y)\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "        # update parameters and zero gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item(), accuracy*100\n",
        "    return train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqB698d6VNb5"
      },
      "outputs": [],
      "source": [
        "def make_validate_fnc(model,loss_fnc):\n",
        "    def validate(X,Y):\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output_logits, output_softmax = model(X)\n",
        "            predictions = torch.argmax(output_softmax,dim=1)\n",
        "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "            loss = loss_fnc(output_logits,Y)\n",
        "        return loss.item(), accuracy*100, predictions\n",
        "    return validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxg0uLMGVNb5"
      },
      "source": [
        "stack data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BRvQ2ajVNb6"
      },
      "outputs": [],
      "source": [
        "X_train = np.stack(mel_train_chunked,axis=0)\n",
        "X_train = np.expand_dims(X_train,2)\n",
        "print('Shape of X_train: ',X_train.shape)\n",
        "X_val = np.stack(mel_val_chunked,axis=0)\n",
        "X_val = np.expand_dims(X_val,2)\n",
        "print('Shape of X_val: ',X_val.shape)\n",
        "X_test = np.stack(mel_test_chunked,axis=0)\n",
        "X_test = np.expand_dims(X_test,2)\n",
        "print('Shape of X_test: ',X_test.shape)\n",
        "\n",
        "del mel_train_chunked\n",
        "del mel_train\n",
        "del mel_val_chunked\n",
        "del mel_val\n",
        "del mel_test_chunked\n",
        "del mel_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWiObjDXVNb6"
      },
      "source": [
        "scale data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll_nAbOxVNb6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "b,t,c,h,w = X_train.shape\n",
        "X_train = np.reshape(X_train, newshape=(b,-1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n",
        "\n",
        "b,t,c,h,w = X_test.shape\n",
        "X_test = np.reshape(X_test, newshape=(b,-1))\n",
        "X_test = scaler.transform(X_test)\n",
        "X_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n",
        "\n",
        "b,t,c,h,w = X_val.shape\n",
        "X_val = np.reshape(X_val, newshape=(b,-1))\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val = np.reshape(X_val, newshape=(b,t,c,h,w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1WtusHjVNb7"
      },
      "source": [
        "train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfB3Eo5QVNb7"
      },
      "outputs": [],
      "source": [
        "EPOCHS=700\n",
        "DATASET_SIZE = X_train.shape[0]\n",
        "BATCH_SIZE = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Selected device is {}'.format(device))\n",
        "model = HybridModel(num_emotions=len(EMOTIONS)).to(device)\n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
        "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
        "\n",
        "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
        "validate = make_validate_fnc(model,loss_fnc)\n",
        "losses=[]\n",
        "val_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    # schuffle data\n",
        "    ind = np.random.permutation(DATASET_SIZE)\n",
        "    X_train = X_train[ind,:,:,:,:]\n",
        "    Y_train = Y_train[ind]\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
        "    for i in range(iters):\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
        "        actual_batch_size = batch_end-batch_start\n",
        "        X = X_train[batch_start:batch_end,:,:,:,:]\n",
        "        Y = Y_train[batch_start:batch_end]\n",
        "        X_tensor = torch.tensor(X,device=device).float()\n",
        "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
        "        loss, acc = train_step(X_tensor,Y_tensor)\n",
        "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
        "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
        "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
        "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
        "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
        "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
        "    losses.append(epoch_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print('')\n",
        "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN4IKmEkVNb7"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn9Vga2ZVNb7"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
        "os.makedirs('models',exist_ok=True)\n",
        "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_model.pt'))\n",
        "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_model.pt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qApL3Ps0VNb8"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk7AyMYeVNb8"
      },
      "outputs": [],
      "source": [
        "LOAD_PATH = '/content/drive/MyDrive/cnn_lstm_model.pt'\n",
        "model = HybridModel(len(EMOTIONS))\n",
        "model.load_state_dict(torch.load(LOAD_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gd8RoBDVNb8"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HGkCDFEVNb8"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
        "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
        "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
        "print(f'Test loss is {test_loss:.3f}')\n",
        "print(f'Test accuracy is {test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bD9yC9jVNb8"
      },
      "source": [
        "confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgmYtuKKVNb9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "predictions = predictions.cpu().numpy()\n",
        "cm = confusion_matrix(Y_test, predictions)\n",
        "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
        "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWGFFPkwVNb9"
      },
      "source": [
        "correlation between emotion intensity and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny6YkOf0VNb9"
      },
      "outputs": [],
      "source": [
        "correct_strong = 0\n",
        "correct_normal = 0\n",
        "wrong_strong = 0\n",
        "wrong_normal = 0\n",
        "for i in range(len(X_test)):\n",
        "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
        "    if Y_test[i] == predictions[i]: # correct prediction\n",
        "        if  intensity == 'normal':\n",
        "            correct_normal += 1\n",
        "        else:\n",
        "            correct_strong += 1\n",
        "    else: # wrong prediction\n",
        "        if intensity == 'normal':\n",
        "            wrong_normal += 1\n",
        "        else:\n",
        "            wrong_strong += 1\n",
        "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
        "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKaJpkouVNb9"
      },
      "source": [
        "correlation between gender and corectness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-GLPzWVVNb-"
      },
      "outputs": [],
      "source": [
        "correct_male = 0\n",
        "correct_female = 0\n",
        "wrong_male = 0\n",
        "wrong_female = 0\n",
        "for i in range(len(X_test)):\n",
        "    gender = data.loc[test_ind[i],'Gender']\n",
        "    if Y_test[i] == predictions[i]: # correct prediction\n",
        "        if  gender == 'male':\n",
        "            correct_male += 1\n",
        "        else:\n",
        "            correct_female += 1\n",
        "    else: # wrong prediction\n",
        "        if gender == 'male':\n",
        "            wrong_male += 1\n",
        "        else:\n",
        "            wrong_female += 1\n",
        "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
        "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkW7JapYVNb-"
      },
      "source": [
        "# Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntNUHNGEVNb-"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses,'b')\n",
        "plt.plot(val_losses,'r')\n",
        "plt.legend(['train loss','val loss'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "stacked_cnn_lstm2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}