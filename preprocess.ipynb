{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPOyvNintUWeVS7LOg2WGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Voice-Emotion-Recognition/blob/main/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing for Sentiment Analysis Recognition**\n",
        "\n",
        "By Dolev Abuhazira and Dor Azaria.\n",
        "\n",
        "\n",
        "For this project, we collected several datasets around the network.\n",
        "Finding those kinds of datasets was challenging because sentiment speech recognition isn’t popular research.\n",
        "Every dataset has a different representation for the same class labels.\n",
        "After a long search on Huggingface and Kaggle, we’ve found the following:\n",
        "\n",
        "|Dataset | Files | Speech Emotions |\n",
        "| -- | -- | -- |\n",
        "| RAVDESS |  60 trials per actor x 24 actors = 1440 files. |calm, happy, sad, angry, fearful, surprised, and disgusted. |\n",
        "| TESS | 200 trials per emotion x7 per gender x2 = 2800 files. |  anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral|\n",
        "| URDU | 100 trials per emotion x4 = 400 files. | Angry, Happy, Neutral, and Sad. |"
      ],
      "metadata": {
        "id": "01LdjfZCXg9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO5JOfpL9hiD",
        "outputId": "29ca7ea3-ee08-40f2-dff1-19e58707410f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/data/; to attempt to forcibly remount, call drive.mount(\"/content/data/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "ru4q5AgG_jpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import pickle\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "pL6DmFwx_i3H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas DataFrame**\n",
        "\n",
        "create a dataframe 'data' - contains the following pattern:\n",
        "\n",
        "| Emotion | Path |\n",
        "| --- | --- |\n",
        "| 0 | '/path/to/wavfile0.wav' |\n",
        "| 1 | '/path/to/wavfile1.wav' |\n",
        "| 2 | '/path/to/wavfile2.wav' |\n",
        "| .. | ... |"
      ],
      "metadata": {
        "id": "LF3nZ02K_rU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(columns=['Emotion', 'Path'])"
      ],
      "metadata": {
        "id": "7BEJhPMwAjDy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Class Distribution***\n",
        "\n",
        "For better accuracy results, we decided to focus on 3 different class labels.\n",
        "Each class is a combination of common emotions.\n",
        "\n",
        "```distributeEmotion``` - For each audio file, we classified its label to one of the three main classes (Positive, Neutral, Negative) and added it into the Panda’s Dataframe by the following form: (Label, File_path).\n",
        "\n",
        "1. **Positive** - a mixture of Happy and Surprise.\n",
        "2. **Neutral** - a mixture of Neutral and Calm.\n",
        "3. **Negative** - a mixture of Anger, Fear, Sad, and Disgust.\n",
        "\n",
        "Each dataset represents different names for the same emotion, for example \"ang\" or \"anger\" or \"a\" represents the same sentiment - Anger."
      ],
      "metadata": {
        "id": "b3VNJ1UGaA7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POSITIVE = 0\n",
        "NEUTRAL = 1\n",
        "NEGATIVE = 2\n",
        "\n",
        "def distributeEmotion(emotion):\n",
        "\n",
        "    if isinstance(emotion, str):\n",
        "      emotion = emotion.lower()\n",
        "\n",
        "    if emotion in {'ang', 'dis', 'fea', 'sad','angry' , 'anger', 'disgust', 'fear', 'fearful', 'sad', 'sadness', 4, 5, 6, 7, 'negative', 's', 'a', 'f'}:\n",
        "      return NEGATIVE\n",
        "\n",
        "    if emotion in {'neu','neutral', 'calm', 1, 2, 'n'}:\n",
        "      return NEUTRAL\n",
        "\n",
        "    if emotion in {'hap', 'happy', 'hapiness', 'ps', 'surprised', 'excited', 'encouraging', 3, 8, 'positive', 'h', 'w'}:\n",
        "      return POSITIVE\n",
        "\n",
        "    return -1\n",
        "        "
      ],
      "metadata": {
        "id": "ftxiHx_XhKPE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***IMPORT DATASETS AUDIO PATH***\n"
      ],
      "metadata": {
        "id": "ug0CEh1UAqPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = ['/content/data/MyDrive/dl/ravdess', '/content/data/MyDrive/dl/tess', '/content/data/MyDrive/dl/urdu']\n",
        "emotion = -1"
      ],
      "metadata": {
        "id": "KcEbuZ_zdeeq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each dataset (RAVDESS / TESS / URDU) we import the audio file path and attach its appropriate label."
      ],
      "metadata": {
        "id": "Uw_T-WFwgBdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ds_path in datasets_path:\n",
        "  for dirname, _, filenames in os.walk(ds_path):\n",
        "      for filename in filenames:\n",
        "          file_path = os.path.join('\\\\', dirname, filename)\n",
        "          \n",
        "          if ds_path == '/content/data/MyDrive/dl/ravdess':\n",
        "              identifiers = filename.split('.')[0].split('-')\n",
        "              emotion = distributeEmotion(int(identifiers[2]))\n",
        "\n",
        "          if ds_path == '/content/data/MyDrive/dl/tess':\n",
        "              identifiers = filename.split('.')[0].split('_')\n",
        "              emotion = distributeEmotion(identifiers[2])\n",
        "\n",
        "          if ds_path == '/content/data/MyDrive/dl/urdu':\n",
        "              identifiers = filename.split('.')[0].split('_')\n",
        "              emotion = distributeEmotion(dirname[10:])\n",
        "\n",
        "          if emotion != -1:\n",
        "              data = data.append( {\"Emotion\": emotion, \"Path\": file_path } , ignore_index=True)"
      ],
      "metadata": {
        "id": "6KV0CG8UApRE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***IMPORT SUMMARY***\n",
        "\n",
        "*   TOTAL - 4,240\n",
        "*   0) POSITIVE - 1184\n",
        "*   1) NEUTRAL - 688\n",
        "*   2) NEGATIVE - 2368\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nwptcHcLpDK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Emotion'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTIXHu5DBZOW",
        "outputId": "adb443cf-820e-42a1-b10c-c06cc640a075"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    2368\n",
              "0    1184\n",
              "1     688\n",
              "Name: Emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SAMPLE & NORMALIZATION***\n",
        "---"
      ],
      "metadata": {
        "id": "S9GlEW1YBU8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using ``manual_seed(0)`` to control sources of randomness that can cause multiple executions of your application to behave differently. And also so that multiple calls to those operations, given the same inputs, will produce the same result.\n",
        "\n",
        "* A ``torch.device`` is an object representing the device on which a torch.Tensor is or will be allocated.\n",
        "\n",
        "* ``WAV2VEC2_ASR_BASE_960H`` to access the model with pretrained weights, and information/helper functions associated the pretrained weights."
      ],
      "metadata": {
        "id": "N_un52Bzg2vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "SAMPLE_RATE = 16000"
      ],
      "metadata": {
        "id": "-2KQiuHfZLRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "svDx6IXvi3lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(features):\n",
        "    for i in range(len(features[0])):\n",
        "        mlist = features[0][i]\n",
        "        features[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1"
      ],
      "metadata": {
        "id": "oMwEplwzBmpo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_padding(path):\n",
        "    signal = np.zeros((int(SAMPLE_RATE*3 ,)))\n",
        "  \n",
        "    waveform, sampling_rate = torchaudio.load(filepath=path, num_frames=SAMPLE_RATE * 3)\n",
        "\n",
        "    waveform = waveform.to(device)\n",
        "    waveform = waveform.detach().cpu().numpy()[0]\n",
        "\n",
        "    if len(waveform) >= 32000 and len(waveform) <= 48000:\n",
        "        signal[:len(waveform)] = waveform\n",
        "\n",
        "        if sampling_rate < 48000: # if there is more to fill\n",
        "          rest = len(signal) - len(waveform) # get the \"rest length\"\n",
        "          filled_list = signal[:len(waveform)] # we don't want to choose zero values, so this list contains non-zero values only.\n",
        "          signal[len(waveform):] = random.choices(filled_list, k=rest) # choose k values from the filled_list\n",
        "          \n",
        "        signal_final = np.array([np.array(signal)])\n",
        "        signal_final = torch.from_numpy(signal_final).to(device)\n",
        "        signal_final = signal_final.type(torch.cuda.FloatTensor).to(device)\n",
        "\n",
        "        return signal_final\n",
        "\n",
        "    return -1"
      ],
      "metadata": {
        "id": "Ub2-T99aO0WR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAMPLE DATA**\n",
        "---\n",
        "EACH SAMPLE SHAPE IS (1, 149, 32)\n",
        "\n"
      ],
      "metadata": {
        "id": "mA9aJZZtBohm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signals = []\n",
        "\n",
        "total_data = len(data)\n",
        "with torch.inference_mode():\n",
        "    for i, file_path in enumerate(data.Path):\n",
        "        tor = resample_padding(file_path)\n",
        "\n",
        "        if isinstance(tor, torch.Tensor):\n",
        "            emission, _ = model(tor)\n",
        "            features = emission.detach().cpu().numpy()\n",
        "            normalize_features(features)\n",
        "            row = (file_path, features, data.iloc[i]['Emotion'])\n",
        "            signals.append(row)\n",
        "\n",
        "        percent = (len(signals) / total_data) * 100\n",
        "        print(\"\\r Processed {}/{} files. ({}%) \".format(len(signals), total_data, int(percent)), end='')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7GkyWe9B4MU",
        "outputId": "3ed9a6e8-178b-4300-fc3d-52d4f2baa1e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 4235/4240 files. (99%) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAVE DATA**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4cjP-RpHB56x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = [0, 0, 0]\n",
        "\n",
        "for tup in signals:\n",
        "  counter[tup[2]] += 1\n",
        "\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "UfIF8PgeZZRI",
        "outputId": "6c064088-c50d-4fa8-deed-35042b6531d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1184, 688, 2363]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_pth = open('/content/dataset444.pth', 'wb')\n",
        "pickle.dump(signals, file_pth)"
      ],
      "metadata": {
        "id": "UG_kipMoB8Yh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}