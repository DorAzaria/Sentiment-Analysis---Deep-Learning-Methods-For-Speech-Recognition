{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SER.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuY6rBplsQguhi8TGQG7nT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Sentiment-Analysis-Deep-Learning-Methods-For-Speech-Recognition/blob/main/SER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Google Drive**"
      ],
      "metadata": {
        "id": "mlSe0XjMBSog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjXCe-ojBQ64",
        "outputId": "b715a94a-8222-4eef-fd24-88ae8a13dcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages**"
      ],
      "metadata": {
        "id": "ocL53VHFBWo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install jiwer\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "wjyhYi9wBaSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Terminal commands**"
      ],
      "metadata": {
        "id": "lT6RdcECBbCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env LC_ALL=C.UTF-8\n",
        "%env LANG=C.UTF-8\n",
        "%env TRANSFORMERS_CACHE=/content/cache\n",
        "%env HF_DATASETS_CACHE=/content/cache\n",
        "%env CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSd2wQwiBgGb",
        "outputId": "d6ac7a6a-767d-4ed5-96a8-5e3b66c66b7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LC_ALL=C.UTF-8\n",
            "env: LANG=C.UTF-8\n",
            "env: TRANSFORMERS_CACHE=/content/cache\n",
            "env: HF_DATASETS_CACHE=/content/cache\n",
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import packages**"
      ],
      "metadata": {
        "id": "-GxaD8dCBjyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import sys\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import torch\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "8AStZiyZBlpk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activate device**"
      ],
      "metadata": {
        "id": "3lO_ewzmB5Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sG8z7pG8B5cG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WAV2VEC2_ASR_BASE_960H - Build “base” wav2vec2 model with an extra linear module. \n",
        "\n",
        "Pre-trained on 960 hours of unlabeled audio from LibriSpeech dataset"
      ],
      "metadata": {
        "id": "Wvha4PF7B_lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)"
      ],
      "metadata": {
        "id": "BSUYmyEYB_6H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling sound method**"
      ],
      "metadata": {
        "id": "LqIK2Th-CXLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise has been changed from 8 to 0\n",
        "DATA_PATH = '../content/drive/MyDrive/audio_speech_actors_01-24'\n",
        "SAMPLE_RATE = 16000\n",
        "signals = []"
      ],
      "metadata": {
        "id": "AtWWuLJ3HVmV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
        "for dirname, _, filenames in os.walk(DATA_PATH):\n",
        "    for filename in filenames:\n",
        "        file_path = os.path.join('/',dirname, filename)\n",
        "        identifiers = filename.split('.')[0].split('-')\n",
        "        emotion = (int(identifiers[2]))\n",
        "        if emotion == 8: # surprise has been changed from 8 to 0\n",
        "            emotion = 0\n",
        "        if int(identifiers[3]) == 1: # intensity (1 = normal, 2 = strong)\n",
        "            emotion_intensity = 'normal' \n",
        "        else: \n",
        "            emotion_intensity = 'strong'\n",
        "        if int(identifiers[6])%2 == 0: # actor id. (even = female, odd = male)\n",
        "            gender = 'female'\n",
        "        else:\n",
        "            gender = 'male'\n",
        "        \n",
        "        data = data.append({\"Emotion\": emotion,\n",
        "                            \"Emotion intensity\": emotion_intensity,\n",
        "                            \"Gender\": gender,\n",
        "                            \"Path\": file_path\n",
        "                             },\n",
        "                             ignore_index = True\n",
        "                          )"
      ],
      "metadata": {
        "id": "Za5KrsUDHTSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of files is {}\".format(len(data)))\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "BXMaMYkgIU3E",
        "outputId": "fb448373-b175-4e1b-d2b6-487db8c9931b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of files is 1440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Emotion Emotion intensity Gender  \\\n",
              "0       4            strong   male   \n",
              "1       4            normal   male   \n",
              "2       4            normal   male   \n",
              "\n",
              "                                                Path  \n",
              "0  /../content/drive/MyDrive/audio_speech_actors_...  \n",
              "1  /../content/drive/MyDrive/audio_speech_actors_...  \n",
              "2  /../content/drive/MyDrive/audio_speech_actors_...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4364c7b-896b-492d-80e6-9d8fd0418bae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion intensity</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>strong</td>\n",
              "      <td>male</td>\n",
              "      <td>/../content/drive/MyDrive/audio_speech_actors_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/../content/drive/MyDrive/audio_speech_actors_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>normal</td>\n",
              "      <td>male</td>\n",
              "      <td>/../content/drive/MyDrive/audio_speech_actors_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4364c7b-896b-492d-80e6-9d8fd0418bae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4364c7b-896b-492d-80e6-9d8fd0418bae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4364c7b-896b-492d-80e6-9d8fd0418bae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_file_to_array_fn(path):\n",
        "    waveform, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
        "    waveform = waveform.to(device)\n",
        "    signal = np.zeros((int(SAMPLE_RATE*3 + 1,)))\n",
        "    signal[:len(waveform)] = waveform\n",
        "    return signal"
      ],
      "metadata": {
        "id": "5fQwMVEzGND_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **InferenceMode** is a new context manager analogous to no_grad to be used when you are certain your operations will have no interactions with autograd (e.g., model training). Code run under this mode gets better performance by disabling view tracking and version counter bumps.\n",
        "\n"
      ],
      "metadata": {
        "id": "VKJ6cky1Cd82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    for i, file_path in enumerate(data.Path):\n",
        "        emission, _ = model(speech_file_to_array_fn(file_path))\n",
        "        signals.append(emission)\n",
        "        print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')"
      ],
      "metadata": {
        "id": "U7eOQcWCF_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyCTCDecoder(torch.nn.Module):\n",
        "    def __init__(self, labels, blank=0):\n",
        "        super().__init__()\n",
        "        self.labels = labels\n",
        "        self.blank = blank\n",
        "\n",
        "    def forward(self, emission: torch.Tensor) -> str:\n",
        "        \"\"\"Given a sequence emission over labels, get the best path string\n",
        "        Args:\n",
        "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
        "\n",
        "        Returns:\n",
        "          str: The resulting transcript\n",
        "        \"\"\"\n",
        "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
        "        indices = torch.unique_consecutive(indices, dim=-1)\n",
        "        indices = [i for i in indices if i != self.blank]\n",
        "        return \"\".join([self.labels[i] for i in indices])"
      ],
      "metadata": {
        "id": "eIObr4A2DKg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
        "transcript = decoder(emission[0])"
      ],
      "metadata": {
        "id": "Fo1SA8EGDOJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcript)\n",
        "IPython.display.Audio((\"/content/drive/MyDrive/simon.wav\"))"
      ],
      "metadata": {
        "id": "aj4Pmw7IDPTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}