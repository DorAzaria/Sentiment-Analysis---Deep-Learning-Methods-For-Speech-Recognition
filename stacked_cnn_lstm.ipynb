{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbRkAw1RVNbn"
      },
      "source": [
        "# Load file names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Filename identifiers***\n",
        "\n",
        "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "\n",
        "* Vocal channel (01 = speech, 02 = song).\n",
        "\n",
        "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "\n",
        "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "\n",
        "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "\n",
        "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "\n",
        "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
      ],
      "metadata": {
        "id": "GIs3QwoOKJzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "20SnQEifVNbq"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "# dor dor \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise has been changed from 8 to 0\n",
        "DATA_PATH = '../content/audio_speech_actors_01-24'\n",
        "SAMPLE_RATE = 48000\n",
        "\n",
        "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
        "for dirname, _, filenames in os.walk(DATA_PATH):\n",
        "    for filename in filenames:\n",
        "        file_path = os.path.join('/kaggle/input/',dirname, filename)\n",
        "        identifiers = filename.split('.')[0].split('-')\n",
        "        emotion = (int(identifiers[2]))\n",
        "        if emotion == 8: # surprise has been changed from 8 to 0\n",
        "            emotion = 0\n",
        "        if int(identifiers[3]) == 1: # intensity (1 = normal, 2 = strong)\n",
        "            emotion_intensity = 'normal' \n",
        "        else: \n",
        "            emotion_intensity = 'strong'\n",
        "        if int(identifiers[6])%2 == 0: # actor id. (even = female, odd = male)\n",
        "            gender = 'female'\n",
        "        else:\n",
        "            gender = 'male'\n",
        "        \n",
        "        data = data.append({\"Emotion\": emotion,\n",
        "                            \"Emotion intensity\": emotion_intensity,\n",
        "                            \"Gender\": gender,\n",
        "                            \"Path\": file_path\n",
        "                             },\n",
        "                             ignore_index = True\n",
        "                          )\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "7BVFtvIBFJna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "Sf5D86-hVNbs",
        "outputId": "04e1bcff-d902-45eb-a9a6-46c618ca87e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of files is 1440\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-18376c73-2137-478f-adc8-37a46add07e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion intensity</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>strong</td>\n",
              "      <td>female</td>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_20/03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>normal</td>\n",
              "      <td>female</td>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_20/03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>strong</td>\n",
              "      <td>female</td>\n",
              "      <td>/content/audio_speech_actors_01-24/Actor_20/03...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18376c73-2137-478f-adc8-37a46add07e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18376c73-2137-478f-adc8-37a46add07e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18376c73-2137-478f-adc8-37a46add07e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  Emotion Emotion intensity  Gender  \\\n",
              "0       4            strong  female   \n",
              "1       7            normal  female   \n",
              "2       3            strong  female   \n",
              "\n",
              "                                                Path  \n",
              "0  /content/audio_speech_actors_01-24/Actor_20/03...  \n",
              "1  /content/audio_speech_actors_01-24/Actor_20/03...  \n",
              "2  /content/audio_speech_actors_01-24/Actor_20/03...  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "print(\"number of files is {}\".format(len(data)))\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwW1ZFlcVNbs"
      },
      "source": [
        "number of examples per Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3ERBf93UVNbt",
        "outputId": "20224496-b9d2-46e5-ab7f-7cdbc0987df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of examples')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
        "ax.set_xticks(ticks=range(8))\n",
        "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
        "ax.set_xlabel('Emotion')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vKaQhFmVNbu"
      },
      "source": [
        "number of examples per gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fJFLgtVeVNbu",
        "outputId": "75f5cf49-b346-441c-efa2-2aac16028e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of examples')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "counts = data['Gender'].value_counts()\n",
        "ax.bar(x=[0,1], height=counts.values)\n",
        "ax.set_xticks(ticks=[0,1])\n",
        "ax.set_xticklabels(list(counts.index))\n",
        "ax.set_xlabel('Gender')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-7T0x-VNbv"
      },
      "source": [
        "number of examples per emotion intensity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "T7MdEBwgVNbv",
        "outputId": "60d32da6-b770-4c67-b901-c6a1da9dc052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of examples')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "counts = data['Emotion intensity'].value_counts()\n",
        "ax.bar(x=[0,1], height=counts.values)\n",
        "ax.set_xticks(ticks=[0,1])\n",
        "ax.set_xticklabels(list(counts.index))\n",
        "ax.set_xlabel('Gender')\n",
        "ax.set_ylabel('Number of examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTzEdCkVNbw"
      },
      "source": [
        "# Load the signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6K6k60yuVNbw",
        "outputId": "65d78179-2023-4ec2-f625-cef31f6ca7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0;32m-> 1184\u001b[0;31m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error opening '/kaggle/input/../content/audio_speech_actors_01-24/Actor_20/03-01-04-02-02-01-20.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8ab8290e7fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msignals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msignal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/../content/audio_speech_actors_01-24/Actor_20/03-01-04-02-02-01-20.wav'"
          ]
        }
      ],
      "source": [
        "mel_spectrograms = []\n",
        "signals = []\n",
        "for i, file_path in enumerate(data.Path):\n",
        "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
        "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
        "    signal[:len(audio)] = audio\n",
        "    signals.append(signal)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
        "signals = np.stack(signals,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQqGSjQLVNbx"
      },
      "source": [
        "# Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3uzoW2cVNbx"
      },
      "outputs": [],
      "source": [
        "X = signals\n",
        "train_ind,test_ind,val_ind = [],[],[]\n",
        "X_train,X_val,X_test = [],[],[]\n",
        "Y_train,Y_val,Y_test = [],[],[]\n",
        "for emotion in range(len(EMOTIONS)):\n",
        "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
        "    emotion_ind = np.random.permutation(emotion_ind)\n",
        "    m = len(emotion_ind)\n",
        "    ind_train = emotion_ind[:int(0.8*m)]\n",
        "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
        "    ind_test = emotion_ind[int(0.9*m):]\n",
        "    X_train.append(X[ind_train,:])\n",
        "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
        "    X_val.append(X[ind_val,:])\n",
        "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
        "    X_test.append(X[ind_test,:])\n",
        "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
        "    train_ind.append(ind_train)\n",
        "    test_ind.append(ind_test)\n",
        "    val_ind.append(ind_val)\n",
        "X_train = np.concatenate(X_train,0)\n",
        "X_val = np.concatenate(X_val,0)\n",
        "X_test = np.concatenate(X_test,0)\n",
        "Y_train = np.concatenate(Y_train,0)\n",
        "Y_val = np.concatenate(Y_val,0)\n",
        "Y_test = np.concatenate(Y_test,0)\n",
        "train_ind = np.concatenate(train_ind,0)\n",
        "val_ind = np.concatenate(val_ind,0)\n",
        "test_ind = np.concatenate(test_ind,0)\n",
        "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
        "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
        "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
        "# check if all are unique\n",
        "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
        "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
        "\n",
        "del X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoAJ6nZzVNbx"
      },
      "source": [
        "# Augment signals by adding AWGN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZBJy6P5VNby"
      },
      "outputs": [],
      "source": [
        "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
        "    signal_len = len(signal)\n",
        "    # Generate White Gaussian noise\n",
        "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
        "    # Normalize signal and noise\n",
        "    norm_constant = 2.0**(num_bits-1)\n",
        "    signal_norm = signal / norm_constant\n",
        "    noise_norm = noise / norm_constant\n",
        "    # Compute signal and noise power\n",
        "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
        "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
        "    # Random SNR: Uniform [15, 30] in dB\n",
        "    target_snr = np.random.randint(snr_low, snr_high)\n",
        "    # Compute K (covariance matrix) for each noise \n",
        "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
        "    K = np.ones((signal_len, augmented_num)) * K  \n",
        "    # Generate noisy signal\n",
        "    return signal + K.T * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgwTVlxJVNby"
      },
      "outputs": [],
      "source": [
        "aug_signals = []\n",
        "aug_labels = []\n",
        "for i in range(X_train.shape[0]):\n",
        "    signal = X_train[i,:]\n",
        "    augmented_signals = addAWGN(signal)\n",
        "    for j in range(augmented_signals.shape[0]):\n",
        "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
        "        aug_signals.append(augmented_signals[j,:])\n",
        "        data = data.append(data.iloc[i], ignore_index=True)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
        "aug_signals = np.stack(aug_signals,axis=0)\n",
        "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
        "aug_labels = np.stack(aug_labels,axis=0)\n",
        "Y_train = np.concatenate([Y_train,aug_labels])\n",
        "print('')\n",
        "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdrZXT5hVNbz"
      },
      "source": [
        "# Calculate mel spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiWEtIfuVNb0"
      },
      "outputs": [],
      "source": [
        "def getMELspectrogram(audio, sample_rate):\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
        "                                              sr=sample_rate,\n",
        "                                              n_fft=1024,\n",
        "                                              win_length = 512,\n",
        "                                              window='hamming',\n",
        "                                              hop_length = 256,\n",
        "                                              n_mels=128,\n",
        "                                              fmax=sample_rate/2\n",
        "                                             )\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return mel_spec_db\n",
        "\n",
        "# test function\n",
        "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
        "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
        "signal[:len(audio)] = audio\n",
        "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
        "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
        "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uekXrF7BVNb0"
      },
      "outputs": [],
      "source": [
        "mel_train = []\n",
        "print(\"Calculatin mel spectrograms for train set\")\n",
        "for i in range(X_train.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_train.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
        "print('')\n",
        "del X_train\n",
        "\n",
        "mel_val = []\n",
        "print(\"Calculatin mel spectrograms for val set\")\n",
        "for i in range(X_val.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_val.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
        "print('')\n",
        "del X_val\n",
        "\n",
        "mel_test = []\n",
        "print(\"Calculatin mel spectrograms for test set\")\n",
        "for i in range(X_test.shape[0]):\n",
        "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
        "    mel_test.append(mel_spectrogram)\n",
        "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
        "print('')\n",
        "del X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNXWko_hVNb1"
      },
      "source": [
        "# Split into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKl22PFbVNb1"
      },
      "outputs": [],
      "source": [
        "def splitIntoChunks(mel_spec,win_size,stride):\n",
        "    t = mel_spec.shape[1]\n",
        "    num_of_chunks = int(t/stride)\n",
        "    chunks = []\n",
        "    for i in range(num_of_chunks):\n",
        "        chunk = mel_spec[:,i*stride:i*stride+win_size]\n",
        "        if chunk.shape[1] == win_size:\n",
        "            chunks.append(chunk)\n",
        "    return np.stack(chunks,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OADpVxkBVNb1"
      },
      "outputs": [],
      "source": [
        "# get chunks\n",
        "# train set\n",
        "mel_train_chunked = []\n",
        "for mel_spec in mel_train:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_train_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
        "# val set\n",
        "mel_val_chunked = []\n",
        "for mel_spec in mel_val:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_val_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
        "# test set\n",
        "mel_test_chunked = []\n",
        "for mel_spec in mel_test:\n",
        "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
        "    mel_test_chunked.append(chunks)\n",
        "print(\"Number of chunks is {}\".format(chunks.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzQy-0JBVNb2"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vapt6LfNVNb3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# BATCH FIRST TimeDistributed layer\n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "        # squash samples and timesteps into a single axis\n",
        "        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n",
        "        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n",
        "        else: # (samples,timesteps,inp1,inp2,inp3)\n",
        "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n",
        "            \n",
        "        y = self.module(x_reshape)\n",
        "        \n",
        "        # we have to reshape Y\n",
        "        if len(x.size()) == 3:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n",
        "        elif len(x.size()) == 4:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n",
        "        else:\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLG4M32EVNb3"
      },
      "outputs": [],
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self,num_emotions):\n",
        "        super().__init__()\n",
        "        # conv block\n",
        "        self.conv2Dblock = nn.Sequential(\n",
        "            # 1. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=1,\n",
        "                                   out_channels=16,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(16)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 2. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=16,\n",
        "                                   out_channels=32,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(32)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 3. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=32,\n",
        "                                   out_channels=64,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(64)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4)),\n",
        "            # 4. conv block\n",
        "            TimeDistributed(nn.Conv2d(in_channels=64,\n",
        "                                   out_channels=128,\n",
        "                                   kernel_size=3,\n",
        "                                   stride=1,\n",
        "                                   padding=1\n",
        "                                  )),\n",
        "            TimeDistributed(nn.BatchNorm2d(128)),\n",
        "            TimeDistributed(nn.ReLU()),\n",
        "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
        "            TimeDistributed(nn.Dropout(p=0.4))\n",
        "        )\n",
        "        # LSTM block\n",
        "        hidden_size = 64\n",
        "        self.lstm = nn.LSTM(input_size=128,hidden_size=hidden_size,bidirectional=False, batch_first=True) \n",
        "        self.dropout_lstm = nn.Dropout(p=0.3)\n",
        "        # Linear softmax layer\n",
        "        self.out_linear = nn.Linear(hidden_size,num_emotions)\n",
        "    def forward(self,x):\n",
        "        conv_embedding = self.conv2Dblock(x)\n",
        "        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n",
        "        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n",
        "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
        "        # lstm_embedding (batch, time, hidden_size)\n",
        "        lstm_output = lstm_embedding[:,-1,:] \n",
        "        output_logits = self.out_linear(lstm_output)\n",
        "        output_softmax = nn.functional.softmax(output_logits,dim=1)\n",
        "        return output_logits, output_softmax\n",
        "                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez_7eMZoVNb4"
      },
      "outputs": [],
      "source": [
        "def loss_fnc(predictions, targets):\n",
        "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo1iRY8yVNb5"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsDMnMs3VNb5"
      },
      "outputs": [],
      "source": [
        "def make_train_step(model, loss_fnc, optimizer):\n",
        "    def train_step(X,Y):\n",
        "        # set model to train mode\n",
        "        model.train()\n",
        "        # forward pass\n",
        "        output_logits, output_softmax = model(X)\n",
        "        predictions = torch.argmax(output_softmax,dim=1)\n",
        "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "        # compute loss\n",
        "        loss = loss_fnc(output_logits, Y)\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "        # update parameters and zero gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item(), accuracy*100\n",
        "    return train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqB698d6VNb5"
      },
      "outputs": [],
      "source": [
        "def make_validate_fnc(model,loss_fnc):\n",
        "    def validate(X,Y):\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output_logits, output_softmax = model(X)\n",
        "            predictions = torch.argmax(output_softmax,dim=1)\n",
        "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "            loss = loss_fnc(output_logits,Y)\n",
        "        return loss.item(), accuracy*100, predictions\n",
        "    return validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxg0uLMGVNb5"
      },
      "source": [
        "stack data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BRvQ2ajVNb6"
      },
      "outputs": [],
      "source": [
        "X_train = np.stack(mel_train_chunked,axis=0)\n",
        "X_train = np.expand_dims(X_train,2)\n",
        "print('Shape of X_train: ',X_train.shape)\n",
        "X_val = np.stack(mel_val_chunked,axis=0)\n",
        "X_val = np.expand_dims(X_val,2)\n",
        "print('Shape of X_val: ',X_val.shape)\n",
        "X_test = np.stack(mel_test_chunked,axis=0)\n",
        "X_test = np.expand_dims(X_test,2)\n",
        "print('Shape of X_test: ',X_test.shape)\n",
        "\n",
        "del mel_train_chunked\n",
        "del mel_train\n",
        "del mel_val_chunked\n",
        "del mel_val\n",
        "del mel_test_chunked\n",
        "del mel_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWiObjDXVNb6"
      },
      "source": [
        "scale data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll_nAbOxVNb6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "b,t,c,h,w = X_train.shape\n",
        "X_train = np.reshape(X_train, newshape=(b,-1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n",
        "\n",
        "b,t,c,h,w = X_test.shape\n",
        "X_test = np.reshape(X_test, newshape=(b,-1))\n",
        "X_test = scaler.transform(X_test)\n",
        "X_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n",
        "\n",
        "b,t,c,h,w = X_val.shape\n",
        "X_val = np.reshape(X_val, newshape=(b,-1))\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val = np.reshape(X_val, newshape=(b,t,c,h,w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1WtusHjVNb7"
      },
      "source": [
        "train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfB3Eo5QVNb7"
      },
      "outputs": [],
      "source": [
        "EPOCHS=700\n",
        "DATASET_SIZE = X_train.shape[0]\n",
        "BATCH_SIZE = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Selected device is {}'.format(device))\n",
        "model = HybridModel(num_emotions=len(EMOTIONS)).to(device)\n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
        "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
        "\n",
        "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
        "validate = make_validate_fnc(model,loss_fnc)\n",
        "losses=[]\n",
        "val_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    # schuffle data\n",
        "    ind = np.random.permutation(DATASET_SIZE)\n",
        "    X_train = X_train[ind,:,:,:,:]\n",
        "    Y_train = Y_train[ind]\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
        "    for i in range(iters):\n",
        "        batch_start = i * BATCH_SIZE\n",
        "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
        "        actual_batch_size = batch_end-batch_start\n",
        "        X = X_train[batch_start:batch_end,:,:,:,:]\n",
        "        Y = Y_train[batch_start:batch_end]\n",
        "        X_tensor = torch.tensor(X,device=device).float()\n",
        "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
        "        loss, acc = train_step(X_tensor,Y_tensor)\n",
        "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
        "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
        "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
        "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
        "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
        "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
        "    losses.append(epoch_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print('')\n",
        "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN4IKmEkVNb7"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn9Vga2ZVNb7"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
        "os.makedirs('models',exist_ok=True)\n",
        "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_model.pt'))\n",
        "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_model.pt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qApL3Ps0VNb8"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk7AyMYeVNb8"
      },
      "outputs": [],
      "source": [
        "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
        "model = HybridModel(len(EMOTIONS))\n",
        "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))\n",
        "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gd8RoBDVNb8"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HGkCDFEVNb8"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
        "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
        "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
        "print(f'Test loss is {test_loss:.3f}')\n",
        "print(f'Test accuracy is {test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bD9yC9jVNb8"
      },
      "source": [
        "confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgmYtuKKVNb9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "predictions = predictions.cpu().numpy()\n",
        "cm = confusion_matrix(Y_test, predictions)\n",
        "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
        "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWGFFPkwVNb9"
      },
      "source": [
        "correlation between emotion intensity and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny6YkOf0VNb9"
      },
      "outputs": [],
      "source": [
        "correct_strong = 0\n",
        "correct_normal = 0\n",
        "wrong_strong = 0\n",
        "wrong_normal = 0\n",
        "for i in range(len(X_test)):\n",
        "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
        "    if Y_test[i] == predictions[i]: # correct prediction\n",
        "        if  intensity == 'normal':\n",
        "            correct_normal += 1\n",
        "        else:\n",
        "            correct_strong += 1\n",
        "    else: # wrong prediction\n",
        "        if intensity == 'normal':\n",
        "            wrong_normal += 1\n",
        "        else:\n",
        "            wrong_strong += 1\n",
        "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
        "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKaJpkouVNb9"
      },
      "source": [
        "correlation between gender and corectness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-GLPzWVVNb-"
      },
      "outputs": [],
      "source": [
        "correct_male = 0\n",
        "correct_female = 0\n",
        "wrong_male = 0\n",
        "wrong_female = 0\n",
        "for i in range(len(X_test)):\n",
        "    gender = data.loc[test_ind[i],'Gender']\n",
        "    if Y_test[i] == predictions[i]: # correct prediction\n",
        "        if  gender == 'male':\n",
        "            correct_male += 1\n",
        "        else:\n",
        "            correct_female += 1\n",
        "    else: # wrong prediction\n",
        "        if gender == 'male':\n",
        "            wrong_male += 1\n",
        "        else:\n",
        "            wrong_female += 1\n",
        "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
        "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkW7JapYVNb-"
      },
      "source": [
        "# Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntNUHNGEVNb-"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses,'b')\n",
        "plt.plot(val_losses,'r')\n",
        "plt.legend(['train loss','val loss'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "stacked-cnn-lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}