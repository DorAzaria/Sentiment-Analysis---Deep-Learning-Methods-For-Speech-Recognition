{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Voice-Emotion-Recognition/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Speach Emotion Recognition***\n",
        "\n",
        "By Dolev Abuhazira and Dor Azaria."
      ],
      "metadata": {
        "id": "5bXeVn3XzUHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685hzZYY0Oua",
        "outputId": "71dde722-7ea9-446d-ebd7-1846d7413ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3h7Ml8w1u_g"
      },
      "source": [
        "***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCj-OhuD1uyS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import mat\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "!sudo apt-get install libportaudio2\n",
        "!sudo apt-get install python-scipy\n",
        "\n",
        "!pip install sounddevice\n",
        "!pip install scipy\n",
        "\n",
        "import sounddevice\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "classes = {0: 'positive', 1:'neutral', 2:'negative'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4QYGCi02Y7"
      },
      "source": [
        "### ***Preprocessed Data***\n",
        "---\n",
        "\n",
        "Data class contains the dataset (for each audio file: audio_path, features, label). ``dataset5.pth`` contains extracted features used by Wav2Vec pretrained model,  the audio files are from RAVDESS, TESS and URDU datasets. - for more details about the preprocess [click here](https://github.com/DorAzaria/Voice-Emotion-Recognition/blob/main/preprocess.ipynb).\n",
        "\n",
        "\n",
        "We split ``self.data`` into random 80% train, 10% test and 10% valid subsets.\n",
        "\n",
        "Each subset (train, test, valid) will be a ``TensorLoader`` type that represents a Python iterable over a dataset.\n",
        "\n",
        "By using ``TensorLoader`` we can easily:\n",
        "1. Activate an automatic batching method (Using ``batch_size=32``).\n",
        "2. Drop the last incomplete batch, if the dataset size is not divisible by the batch size (Using ``drop_last=True``).\n",
        "3. To have the data reshuffled at every epoch (Using ``shuffle=True``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "820sVvYW1E5o"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        file_handler = open('/content/data/MyDrive/dl/dataset5.pth', 'rb')\n",
        "        self.data = pickle.load(file_handler)\n",
        "\n",
        "        x_dataset = [embedding[1] for embedding in self.data]\n",
        "        y_dataset = [label[2] for label in self.data]\n",
        "\n",
        "        #[80% train, 10% valid, 10% test]\n",
        "        train_x, rem_x, train_y, rem_y = train_test_split(np.array(x_dataset), np.array(y_dataset), train_size=0.80) \n",
        "        valid_x, test_x, valid_y, test_y = train_test_split(rem_x, rem_y, test_size=0.5)\n",
        "\n",
        "        # Convert numpy to torch.Tensor type\n",
        "        self.train_x = torch.from_numpy(train_x)\n",
        "        self.train_y = torch.from_numpy(train_y)\n",
        "\n",
        "        self.valid_x = torch.from_numpy(valid_x)\n",
        "        self.valid_y = torch.from_numpy(valid_y)\n",
        "\n",
        "        self.test_x = torch.from_numpy(test_x)\n",
        "        self.test_y = torch.from_numpy(test_y)\n",
        "\n",
        "        # Create a DataLoader for each subset (train, valid, test)\n",
        "        torch_train = TensorDataset(self.train_x, self.train_y)\n",
        "        torch_valid = TensorDataset(self.valid_x, self.valid_y)\n",
        "        torch_test = TensorDataset(self.test_x, self.test_y)\n",
        "\n",
        "        self.train_loader = DataLoader(torch_train, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.valid_loader = DataLoader(torch_valid, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.test_loader = DataLoader(torch_test, batch_size=32, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot**\n",
        "\n",
        "Prints the train loss vs. valid loss when an earlystopping occurs."
      ],
      "metadata": {
        "id": "Ad32J_Fi1gK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotEarlyStopping(train_loss, valid_loss):\n",
        "  epochs_x_axis = np.linspace(1, len(train_loss), len(train_loss)).astype(int)\n",
        "  plt.plot(epochs_x_axis, train_loss)\n",
        "  plt.plot(epochs_x_axis, valid_loss)\n",
        "  plt.title('Train VS Valid Loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
        "  plt.savefig(\"model_plot.png\")"
      ],
      "metadata": {
        "id": "ZxC8IagDCrDZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9NoyVXt2F_6"
      },
      "source": [
        "### ***ConvNet Class Model***\n",
        "---\n",
        "\n",
        "The ``ConvNet`` model is a convolutional neural network. The model subclass ``torch.nn.Module`` which is a base class for all neural network modules.\n",
        "The model architecture using ``BatchNorm2d()``,  ``MaxPool2d()`` , ``ReLU()``and more methods from ``nn.Module``.\n",
        "\n",
        "The input size of the network is (1, 149, 32)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "56-6NfI62iiW"
      },
      "outputs": [],
      "source": [
        "DROP_OUT = 0.8\n",
        "NUM_OF_CLASSES = 3\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.epochs = 300\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # Model Architecture\n",
        "        self.first_conv = nn.Conv2d(1, 96, kernel_size=(5, 5), padding=1) # (96, 147, 30)\n",
        "        self.first_bn = nn.BatchNorm2d(96)\n",
        "        self.first_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2)) # (96, 73, 14)\n",
        "\n",
        "        self.second_conv = nn.Conv2d(96, 256, kernel_size=(5, 5), padding=1) # (256, 71, 12)\n",
        "        self.second_bn = nn.BatchNorm2d(256)\n",
        "        self.second_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1)) # (256, 69, 10)\n",
        "\n",
        "        self.third_conv = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=1) # (384, 69, 10 )\n",
        "        self.third_bn = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.forth_conv = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.forth_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fifth_conv = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.fifth_bn = nn.BatchNorm2d(256)\n",
        "        self.fifth_polling = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1)) # (256, 68, 9)\n",
        "\n",
        "        self.sixth_conv = nn.Conv2d(256, 64, kernel_size=(2, 2), padding=1) # (64, 69, 10)\n",
        "\n",
        "        self.seventh_conv = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1) # (64, 69, 10)\n",
        "        self.seventh_polling = nn.MaxPool2d(kernel_size=(2,2), stride=(2, 2)) # (64, 34, 5)\n",
        "\n",
        "        self.eighth_conv = nn.Conv2d(64, 32, kernel_size=(3,3), padding=1) # (32, 34, 5)\n",
        "        self.first_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.avg_polling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.first_dense = nn.Linear(32, 1024)\n",
        "        self.second_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.second_dense = nn.Linear(1024, NUM_OF_CLASSES)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = nn.ReLU()(self.first_conv(X))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.first_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.second_conv(x))\n",
        "        x = self.second_bn(x)\n",
        "        x = self.second_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.third_conv(x))\n",
        "        x = self.third_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.forth_conv(x))\n",
        "        x = self.forth_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.fifth_conv(x))\n",
        "        x = self.fifth_bn(x)\n",
        "        x = self.fifth_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.sixth_conv(x))\n",
        "\n",
        "        x = nn.ReLU()(self.seventh_conv(x))\n",
        "        x = self.seventh_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.eighth_conv(x))\n",
        "\n",
        "        x = self.first_drop(x)\n",
        "        x = self.avg_polling(x)\n",
        "\n",
        "        x = x.view(-1, x.shape[1])  # output channel for flatten before entering the dense layer\n",
        "\n",
        "        x = nn.ReLU()(self.first_dense(x))\n",
        "        x = self.second_drop(x)\n",
        "\n",
        "        x = self.second_dense(x)\n",
        "        y = nn.LogSoftmax(dim=1)(x)  # consider using Log-Softmax\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_epochs(self):\n",
        "        return self.epochs\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.learning_rate\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpaZ7PlR3CRK"
      },
      "source": [
        "# **TEST**\n",
        "---\n",
        "\n",
        "* ``torch.max()`` returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oO0UNKVK2-3S"
      },
      "outputs": [],
      "source": [
        "def test(convnet_model, dataset):\n",
        "\n",
        "    example_class = [0] * len(dataset.test_y) \n",
        "    text_results = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        example_index = 0\n",
        "        label_correct = [0, 0 , 0]\n",
        "        label_samples = [0, 0 , 0]\n",
        "        \n",
        "        for embedding, labels in dataset.test_loader:\n",
        "            \n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            embedding = embedding.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = convnet_model(embedding)\n",
        "            _, predicted = torch.max(input= outputs, dim=1)\n",
        "\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            for i in range(convnet_model.batch_size):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "\n",
        "                if label == pred:\n",
        "                    label_correct[label] += 1\n",
        "\n",
        "                label_samples[label] += 1\n",
        "                example_class[example_index] = pred.cpu()\n",
        "                example_index += 1\n",
        "\n",
        "        acc = 100.0 * total_correct / len(dataset.test_y)\n",
        "        text_results.append(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "        for i in range(3):\n",
        "            acc = 100.0 * label_correct[i] / label_samples[i]\n",
        "            text_results.append(f'Accuracy of {classes[i]}: {acc} %')\n",
        "        \n",
        "        return example_class, text_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tq7sCSuj_I8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(convnet_model, dataset, loss_function):\n",
        "    # Settings\n",
        "    model.eval()\n",
        "    loss_total = 0\n",
        "\n",
        "    # Test validation data\n",
        "    with torch.no_grad():\n",
        "        for embedding, labels in dataset.valid_loader:\n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "\n",
        "            outputs = convnet_model(embedding)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_total += loss.item()\n",
        "\n",
        "    return loss_total / len(dataset.valid_loader)\n"
      ],
      "metadata": {
        "id": "z2IV8w_o_Iu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**\n",
        "---\n"
      ],
      "metadata": {
        "id": "Fmjphp7Uc8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(convnet_model, dataset):\n",
        "    save_counter=0\n",
        "    loss_vals_train = []\n",
        "    loss_valid = []\n",
        "\n",
        "    optimizer = torch.optim.Adam(convnet_model.parameters(), lr=convnet_model.learning_rate, weight_decay=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.99, 3.0, 1.0])).to(device) # [1184, 688, 2363] --> 4235 examples\n",
        "    \n",
        "    n_total_steps = len(dataset.train_loader)\n",
        "\n",
        "    # Early stopping\n",
        "    the_last_loss = 100\n",
        "    patience = 0\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(convnet_model.get_epochs()):\n",
        "\n",
        "        convnet_model.train()\n",
        "        epoch_acc = []\n",
        "        epoch_loss = []\n",
        "\n",
        "        for i, (embedding, labels) in enumerate(dataset.train_loader):\n",
        "\n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "        \n",
        "            labels = labels.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = convnet_model.forward(embedding)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            if i == 90:\n",
        "                print(f'Epoch [{epoch + 1}/{convnet_model.epochs}], Loss: {loss.item():.4f}')\n",
        "                \n",
        "        loss_vals_train.append(sum(epoch_loss)/len(epoch_loss))\n",
        "\n",
        "        _ , res , test_acc = test(convnet_model, dataset)\n",
        "\n",
        "        #### EARLY STOPPING ~~~~~~~~~~~~~~~~~~~\n",
        "        the_current_loss = validation(convnet_model, dataset, criterion)\n",
        "        loss_valid.append(the_current_loss)\n",
        "        print(res)\n",
        "        if the_current_loss > loss_vals_train[-1]:\n",
        "            trigger_times += 1\n",
        "            print('trigger times:', trigger_times)\n",
        "            # if loss_vals_train[-1] < 0.7:\n",
        "            torch.save(cnn, f\"/content/model{save_counter}.pth\")\n",
        "            plotEarlyStopping(loss_vals_train,loss_valid, f\"/content/model{save_counter}.png\")\n",
        "            print(f'saved model{save_counter}.pth')\n",
        "            save_counter += 1\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            trigger_times = 0\n",
        "\n",
        "        the_last_loss = the_current_loss\n",
        "        ##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n",
        "    # plotEarlyStopping(loss_vals_train,loss_valid)"
      ],
      "metadata": {
        "id": "PasvGTEAc5AE"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdGGMiq35tS"
      },
      "source": [
        "# **Main**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQMhz2iXxNQ"
      },
      "source": [
        "## NORM AND INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj2SqEan3-Gz"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "\n",
        "\n",
        "def inference(file_name):\n",
        "    SAMPLE_RATE = 16000\n",
        "    waveform, sample_rate = torchaudio.load(filepath=file_name,  num_frames=SAMPLE_RATE * 3)\n",
        "    # waveform = waveform.view(1, 96000)\n",
        "    waveform = waveform.to(device)\n",
        "    \n",
        "    if (len(waveform[0]) < 48000):\n",
        "        print(f'less than 3 seconds: {file_name}')\n",
        "\n",
        "    if sample_rate != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        embedding, _ = model(waveform)\n",
        "\n",
        "    return embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "def Norm(X):\n",
        "    embedding = X.detach().cpu().numpy()\n",
        "    for i in range(len(embedding)):\n",
        "        mlist = embedding[0][i]\n",
        "        embedding[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1\n",
        "\n",
        "    return torch.from_numpy(embedding).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCDZ2CSE4Mit"
      },
      "source": [
        "## START TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOYlypNI4RD2"
      },
      "outputs": [],
      "source": [
        "dataset = Data()\n",
        "cnn = ConvNet()\n",
        "cnn.to(device)\n",
        "train_model(cnn, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.distrib)\n",
        "print(dataset.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbKwZMiTOgIi",
        "outputId": "70e8a35e-46ab-4329-84f6-2a05c88cce2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[822, 489, 1653]\n",
            "[2.010948905109489, 3.3803680981595092, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn, \"/content/data/MyDrive/dl/model208.pth\")"
      ],
      "metadata": {
        "id": "JEZvqhOZbHFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_import = torch.load(\"/content/data/MyDrive/dl/model208.pth\")"
      ],
      "metadata": {
        "id": "mrUVc4DLlOTz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWNPkZ04UCp"
      },
      "source": [
        "## START TEST\n",
        "\n",
        "\n",
        "Accuracy of the network: 70.4326923076923 %\n",
        "\n",
        "Accuracy of positive: 72.88135593220339 %\n",
        "\n",
        "Accuracy of neutral: 81.48148148148148 %\n",
        "\n",
        "Accuracy of negative: 66.80327868852459 %\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2c1Cu8n2Wv",
        "outputId": "da8b27fd-d7a8-4f20-bf38-e8a1787fe57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 71.93396226415095 %\n",
            "Accuracy of positive: 65.0 %\n",
            "Accuracy of neutral: 80.88235294117646 %\n",
            "Accuracy of negative: 74.59677419354838 %\n"
          ]
        }
      ],
      "source": [
        "dataset = Data()\n",
        "n_class, results  = test(model_import, dataset)\n",
        "\n",
        "for text in results:\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display the confusion matrix as a heatmap\n",
        "arr = confusion_matrix(dataset.test_y.detach().cpu().numpy(), n_class)\n",
        "class_names = ['Positive', 'Neutral', ' Negative']\n",
        "print(arr)\n",
        "df_cm = pd.DataFrame(arr, class_names, class_names)\n",
        "plt.figure(figsize = (9,6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "b3Ok6cs_J9K_",
        "outputId": "f89c47d2-2388-4f2f-f114-5fb8d52ad682"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 77  17  15]\n",
            " [ 10  58   8]\n",
            " [ 45  35 159]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awUU1AREn3fk"
      },
      "source": [
        "## INFERENCE\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(y):\n",
        "  predict = [np.exp(c) for c in y]\n",
        "  max = np.argmax(predict)\n",
        "  print(f'Predicted: {classes[max].capitalize()}')\n",
        "  print(f'Positive: {round(predict[0][0]*100, 4)}%')\n",
        "  print(f'Neutral: {round(predict[0][1]*100, 4)}%')\n",
        "  print(f'Negative: {round(predict[0][2]*100, 4)}%')"
      ],
      "metadata": {
        "id": "Ph6iOQImBFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)."
      ],
      "metadata": {
        "id": "n9w_o2lLtPYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-sKsHj4ZwS",
        "outputId": "cdc09959-a871-435a-8830-6a0a37759d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Negative\n",
            "Positive: 10.478%\n",
            "Neutral: 0.621%\n",
            "Negative: 88.9009%\n"
          ]
        }
      ],
      "source": [
        "# NEGATIVE TRUTH\n",
        "inf_X = inference('/content/data/MyDrive/dl/ravdess/Actor_01/03-01-05-01-01-02-01.wav')\n",
        "X = Norm(inf_X)\n",
        "y = cnn.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "print_results(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEUTRAL TRUTH\n",
        "inf_X = inference('/content/data/MyDrive/dl/ravdess/Actor_03/03-01-01-01-02-01-03.wav')\n",
        "X = Norm(inf_X)\n",
        "y = cnn.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "print_results(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUyOU22cS6oq",
        "outputId": "64922009-82a5-4078-9070-8b530bb5f763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Neutral\n",
            "Positive: 32.7862%\n",
            "Neutral: 34.0334%\n",
            "Negative: 33.1804%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POSITIVE TRUTH\n",
        "inf_X = inference('/content/data/MyDrive/dl/ravdess/Actor_03/03-01-03-01-02-01-03.wav')\n",
        "X = Norm(inf_X)\n",
        "y = cnn.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "print_results(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywB6f5iUS7C5",
        "outputId": "5dbe54bc-c9da-40a1-86a3-047f734ccda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Positive\n",
            "Positive: 40.8426%\n",
            "Neutral: 27.3688%\n",
            "Negative: 31.7885%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "train_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}