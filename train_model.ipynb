{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOrZsP4CKihNvJ/YN7UagXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Sentiment-Analysis-Deep-Learning-Methods-For-Speech-Recognition/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685hzZYY0Oua",
        "outputId": "51d8699d-7a5a-4f3d-8d74-86c7776d92cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**\n",
        "---"
      ],
      "metadata": {
        "id": "W3h7Ml8w1u_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import torchaudio\n",
        "from numpy import mat"
      ],
      "metadata": {
        "id": "jCj-OhuD1uyS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESS**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lS4QYGCi02Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        file_handler = open('/content/data/MyDrive/dl/dataset1.pth', 'rb')\n",
        "        data = pickle.load(file_handler)\n",
        "\n",
        "        x_dataset = [embedding[1] for embedding in data]\n",
        "        y_dataset = [label[2] for label in data]\n",
        "        train_x, test_x, train_y, test_y = train_test_split(np.array(x_dataset), np.array(y_dataset), test_size=0.20)\n",
        "        train_x = torch.from_numpy(train_x)\n",
        "        train_y = torch.from_numpy(train_y)\n",
        "        torch_train = TensorDataset(train_x, train_y)\n",
        "\n",
        "        test_x = torch.from_numpy(test_x)\n",
        "        test_y = torch.from_numpy(test_y)\n",
        "        torch_test = TensorDataset(test_x, test_y)\n",
        "\n",
        "        self.train_loader = DataLoader(torch_train, batch_size=28, drop_last=True, shuffle=True)\n",
        "        self.test_loader = DataLoader(torch_test, batch_size=28, drop_last=True, shuffle=False)"
      ],
      "metadata": {
        "id": "820sVvYW1E5o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "I9NoyVXt2F_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DROP_OUT = 0.5\n",
        "NUM_OF_CLASSES = 3\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_of_classes, dataset):\n",
        "        super().__init__()\n",
        "        # Hyper parameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 28\n",
        "        self.learning_rate = 0.001\n",
        "        self.dataset = dataset\n",
        "        # Model Architecture\n",
        "        self.first_conv = nn.Conv2d(1, 96, kernel_size=(5, 5), padding=1)\n",
        "        self.first_bn = nn.BatchNorm2d(96)\n",
        "        self.first_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
        "\n",
        "        self.second_conv = nn.Conv2d(96, 256, kernel_size=(5, 5), padding=1)\n",
        "        self.second_bn = nn.BatchNorm2d(256)\n",
        "        self.second_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1))\n",
        "\n",
        "        self.third_conv = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=1)\n",
        "        self.third_bn = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.forth_conv = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1)\n",
        "        self.forth_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fifth_conv = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1)\n",
        "        self.fifth_bn = nn.BatchNorm2d(256)\n",
        "        self.fifth_polling = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
        "\n",
        "        self.sixth_conv = nn.Conv2d(256, 64, kernel_size=(2, 2), padding=1)\n",
        "        self.first_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.avg_polling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.first_dense = nn.Linear(64, 1024)\n",
        "        self.second_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.second_dense = nn.Linear(1024, num_of_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = nn.ReLU()(self.first_conv(X))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.first_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.second_conv(x))\n",
        "        x = self.second_bn(x)\n",
        "        x = self.second_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.third_conv(x))\n",
        "        x = self.third_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.forth_conv(x))\n",
        "        x = self.forth_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.fifth_conv(x))\n",
        "        x = self.fifth_bn(x)\n",
        "        x = self.fifth_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.sixth_conv(x))\n",
        "        x = self.first_drop(x)\n",
        "        x = self.avg_polling(x)\n",
        "\n",
        "        x = x.view(-1, x.shape[1])  # output channel for flatten before entering the dense layer\n",
        "\n",
        "        x = nn.ReLU()(self.first_dense(x))\n",
        "        x = self.second_drop(x)\n",
        "\n",
        "        x = self.second_dense(x)\n",
        "        y = nn.LogSoftmax(dim=1)(x)  # consider using Log-Softmax\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_epochs(self):\n",
        "        return self.epochs\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.learning_rate\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([2.103336921, 3.203278689, 1]))\n",
        "\n",
        "        n_total_steps = len(self.dataset.train_loader)\n",
        "\n",
        "        for epoch in range(self.get_epochs()):\n",
        "            for i, (embedding, labels) in enumerate(self.dataset.train_loader):\n",
        "\n",
        "                embedding = embedding.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "                labels.to(device)\n",
        "                \n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.forward(embedding)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if i == 86:\n",
        "                    print(f'Epoch [{epoch + 1}/{self.epochs}], Step [{i + 1}/{n_total_steps}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "56-6NfI62iiW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fpaZ7PlR3CRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class TestConvNet:\n",
        "    def __init__(self, model, dataset):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.results = []\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_samples = 0\n",
        "            n_class_correct = [0 for i in range(3)]\n",
        "            n_class_samples = [0 for i in range(3)]\n",
        "            for embedding, labels in self.dataset.test_loader:\n",
        "\n",
        "                embedding = labels.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "\n",
        "                outputs = self.model.forward(embedding)\n",
        "\n",
        "                # max returns (value ,index)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                n_samples += labels.size(0)\n",
        "                n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                for i in range(self.model.batch_size):\n",
        "                    label = labels[i]\n",
        "                    pred = predicted[i]\n",
        "                    if label == pred:\n",
        "                        n_class_correct[label] += 1\n",
        "                    n_class_samples[label] += 1\n",
        "\n",
        "            acc = 100.0 * n_correct / n_samples\n",
        "            print(f'Accuracy of the network: {acc} %')\n",
        "            self.results.append(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "            for i in range(3):\n",
        "                acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "                print(f'Accuracy of {self.dataset.classes[i]}: {acc} %')\n",
        "                self.results.append(f'Accuracy of {self.dataset.classes[i]}: {acc} %')\n",
        "\n",
        "        saved_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
        "        file_name = 'result.txt'\n",
        "        directory = '/content/data/' + str(saved_time)\n",
        "        os.mkdir(directory)\n",
        "\n",
        "        with open(directory + \"/\" + file_name, 'w') as f:\n",
        "            for line in self.results:\n",
        "                f.write(line)\n",
        "                f.write('\\n')\n",
        "\n",
        "        torch.save(self.model, directory + \"/model.pth\")\n"
      ],
      "metadata": {
        "id": "oO0UNKVK2-3S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FbdGGMiq35tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NORM AND INFERENCE"
      ],
      "metadata": {
        "id": "McQMhz2iXxNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "\n",
        "\n",
        "def Norm(X):\n",
        "    embedding = X.detach().cpu().numpy()\n",
        "    for i in range(len(embedding)):\n",
        "        mlist = embedding[0][i]\n",
        "        embedding[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1\n",
        "        if embedding[0][i] < -1 or embedding[0][i] > 1:\n",
        "            print(\"NISHBAR HAZAIN\")\n",
        "            break\n",
        "    return torch.from_numpy(embedding)\n",
        "\n",
        "\n",
        "def recording(name):\n",
        "    # import sounddevice\n",
        "    # # from scipy.io.wavefile import write\n",
        "    # filename = name\n",
        "    # fps = 16000\n",
        "    # duration = 3\n",
        "    # print(\"Recording ..\")\n",
        "    # recording = sounddevice.rec(int(duration * fps), samplerate = fps, channels = 2)\n",
        "    # sounddevice.wait()\n",
        "    # print(\"Done.\")\n",
        "    # write(filename, fps, recording)\n",
        "    # return filename + \".wav\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def inference(file_name):\n",
        "    waveform, sample_rate = torchaudio.load(recording(file_name))\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    if sample_rate != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        embedding, _ = model(waveform)\n",
        "\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "sj2SqEan3-Gz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## START TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MCDZ2CSE4Mit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aer_dataset = Data()\n",
        "cnn = ConvNet(3, aer_dataset)\n",
        "cnn.train_model()"
      ],
      "metadata": {
        "id": "SOYlypNI4RD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## START TEST\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LTWNPkZ04UCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = TestConvNet(cnn, aer_dataset)\n",
        "test.test()\n",
        "X = Norm(inference(\"dor_angry\"))\n",
        "predict = [mat.exp(c) for c in cnn.forward(X)]"
      ],
      "metadata": {
        "id": "oH-sKsHj4ZwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}