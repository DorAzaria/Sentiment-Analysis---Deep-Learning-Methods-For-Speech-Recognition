{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Sentiment-Analysis-Deep-Learning-Methods-For-Speech-Recognition/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685hzZYY0Oua",
        "outputId": "cec0e188-26cb-425e-d086-2bcac78724bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/data/; to attempt to forcibly remount, call drive.mount(\"/content/data/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3h7Ml8w1u_g"
      },
      "source": [
        "# **IMPORTS**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCj-OhuD1uyS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import torchaudio\n",
        "from numpy import mat\n",
        "\n",
        "!sudo apt-get install libportaudio2\n",
        "!sudo apt-get install python-scipy\n",
        "\n",
        "!pip install sounddevice\n",
        "!pip install scipy\n",
        "\n",
        "import sounddevice\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4QYGCi02Y7"
      },
      "source": [
        "# **PREPROCESS**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "820sVvYW1E5o"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        file_handler = open('/content/data/MyDrive/dl/dataset2.pth', 'rb')\n",
        "        self.data = pickle.load(file_handler)\n",
        "        x_dataset = [embedding[1] for embedding in self.data]\n",
        "        y_dataset = [label[2] for label in self.data]\n",
        "        train_x, test_x, train_y, test_y = train_test_split(np.array(x_dataset), np.array(y_dataset), test_size=0.30)\n",
        "        self.train_x = torch.from_numpy(train_x)\n",
        "        self.train_y = torch.from_numpy(train_y)\n",
        "        torch_train = TensorDataset(self.train_x, self.train_y)\n",
        "        \n",
        "        self.test_x = torch.from_numpy(test_x)\n",
        "        self.test_y = torch.from_numpy(test_y)\n",
        "        torch_test = TensorDataset(self.test_x, self.test_y)\n",
        "        \n",
        "\n",
        "        self.train_loader = DataLoader(torch_train, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.test_loader = DataLoader(torch_test, batch_size=32, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9NoyVXt2F_6"
      },
      "source": [
        "# **TRAIN**\n",
        "\n",
        "1, 149, 32\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "56-6NfI62iiW"
      },
      "outputs": [],
      "source": [
        "DROP_OUT = 0.5\n",
        "NUM_OF_CLASSES = 3\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_of_classes, dataset):\n",
        "        super().__init__()\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.epochs = 300\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 0.0001\n",
        "        self.dataset = dataset\n",
        "\n",
        "        # Model Architecture\n",
        "        self.first_conv = nn.Conv2d(1, 96, kernel_size=(5, 5), padding=1) # (96, 147, 30)\n",
        "        self.first_bn = nn.BatchNorm2d(96)\n",
        "        self.first_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2)) # (96, 73, 14)\n",
        "\n",
        "        self.second_conv = nn.Conv2d(96, 256, kernel_size=(5, 5), padding=1) # (256, 71, 12)\n",
        "        self.second_bn = nn.BatchNorm2d(256)\n",
        "        self.second_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1)) # (256, 69, 10)\n",
        "\n",
        "        self.third_conv = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=1) # (384, 69, 10 )\n",
        "        self.third_bn = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.forth_conv = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.forth_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fifth_conv = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.fifth_bn = nn.BatchNorm2d(256)\n",
        "        self.fifth_polling = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1)) # (256, 68, 9)\n",
        "\n",
        "        self.sixth_conv = nn.Conv2d(256, 64, kernel_size=(2, 2), padding=1) # (64, 69, 10)\n",
        "\n",
        "        self.seventh_conv = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1) # (64, 69, 10)\n",
        "        self.seventh_polling = nn.MaxPool2d(kernel_size=(2,2), stride=(2, 2)) # (64, 34, 5)\n",
        "\n",
        "        self.eighth_conv = nn.Conv2d(64, 32, kernel_size=(3,3), padding=1) # (32, 34, 5)\n",
        "        self.first_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.avg_polling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.first_dense = nn.Linear(32, 1024)\n",
        "        self.second_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.second_dense = nn.Linear(1024, num_of_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = nn.ReLU()(self.first_conv(X))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.first_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.second_conv(x))\n",
        "        x = self.second_bn(x)\n",
        "        x = self.second_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.third_conv(x))\n",
        "        x = self.third_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.forth_conv(x))\n",
        "        x = self.forth_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.fifth_conv(x))\n",
        "        x = self.fifth_bn(x)\n",
        "        x = self.fifth_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.sixth_conv(x))\n",
        "\n",
        "        x = nn.ReLU()(self.seventh_conv(x))\n",
        "        x = self.seventh_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.eighth_conv(x))\n",
        "\n",
        "        x = self.first_drop(x)\n",
        "        x = self.avg_polling(x)\n",
        "\n",
        "        x = x.view(-1, x.shape[1])  # output channel for flatten before entering the dense layer\n",
        "\n",
        "        x = nn.ReLU()(self.first_dense(x))\n",
        "        x = self.second_drop(x)\n",
        "\n",
        "        x = self.second_dense(x)\n",
        "        y = nn.LogSoftmax(dim=1)(x)  # consider using Log-Softmax\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_epochs(self):\n",
        "        return self.epochs\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.learning_rate\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size\n",
        "    \n",
        "    def train_model(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
        "        # [3304, 2895, 9004] --> 15k examples\n",
        "        # [1184, 688, 2363] --> 4235 examples\n",
        "        # criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([2.72518159806, 3.11018998273, 1])).to(device)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.99, 5.43, 1])).to(device)\n",
        "\n",
        "        n_total_steps = len(self.dataset.train_loader)\n",
        "\n",
        "        for epoch in range(self.get_epochs()):\n",
        "            for i, (embedding, labels) in enumerate(self.dataset.train_loader):\n",
        "\n",
        "                embedding = embedding.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "            \n",
        "                labels = labels.to(device)\n",
        "                embedding = embedding.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.forward(embedding)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                if i == 74:\n",
        "                    print(f'Epoch [{epoch + 1}/{self.epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpaZ7PlR3CRK"
      },
      "source": [
        "# **TEST**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oO0UNKVK2-3S"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classes = {0: 'positive', 1:'neutral', 2:'negative'}\n",
        "\n",
        "class TestConvNet:\n",
        "    def __init__(self, model, dataset):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.results = []\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_samples = 0\n",
        "            n_class_correct = [0 for i in range(3)]\n",
        "            n_class_samples = [0 for i in range(3)]\n",
        "            \n",
        "            n_class = [0 for i in range(len(self.dataset.test_y))]\n",
        "            j = 0\n",
        "            \n",
        "            \n",
        "            for embedding, labels in self.dataset.test_loader:\n",
        "                \n",
        "                embedding = embedding.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "                labels = labels.to(device)\n",
        "                embedding = embedding.to(device)\n",
        "                outputs = self.model(embedding)\n",
        "\n",
        "                # max returns (value ,index)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                n_samples += labels.size(0)\n",
        "                n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                for i in range(self.model.batch_size):\n",
        "                    label = labels[i]\n",
        "                    pred = predicted[i]\n",
        "                    if label == pred:\n",
        "                        n_class_correct[label] += 1\n",
        "                    n_class_samples[label] += 1\n",
        "                    n_class[j] = pred.view(-1).detach().cpu().numpy()[0]\n",
        "                    j += 1\n",
        "\n",
        "            acc = 100.0 * n_correct / n_samples\n",
        "            print(f'Accuracy of the network: {acc} %')\n",
        "            self.results.append(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "            for i in range(3):\n",
        "                acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "                print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "                self.results.append(f'Accuracy of {classes[i]}: {acc} %')\n",
        "            \n",
        "            return n_class\n",
        "        # saved_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
        "        # file_name = 'result.txt'\n",
        "        # directory = '/content/data/' + str(saved_time)\n",
        "        # os.mkdir(directory)\n",
        "\n",
        "        # with open(directory + \"/\" + file_name, 'w') as f:\n",
        "        #     for line in self.results:\n",
        "        #         f.write(line)\n",
        "        #         f.write('\\n')\n",
        "\n",
        "        # torch.save(self.model, directory + \"/model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdGGMiq35tS"
      },
      "source": [
        "# **Main**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQMhz2iXxNQ"
      },
      "source": [
        "## NORM AND INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sj2SqEan3-Gz"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "\n",
        "\n",
        "def inference(file_name):\n",
        "    SAMPLE_RATE = 16000\n",
        "    waveform, sample_rate = torchaudio.load(filepath=file_name,  num_frames=SAMPLE_RATE * 3)\n",
        "    waveform = waveform.view(1, 96000)\n",
        "    waveform = waveform.to(device)\n",
        "    \n",
        "    if (len(waveform[0]) < 48000):\n",
        "        print(f'less than 3 seconds: {file_name}')\n",
        "\n",
        "    if sample_rate != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        embedding, _ = model(waveform)\n",
        "\n",
        "    return embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "def Norm(X):\n",
        "    embedding = X.detach().cpu().numpy()\n",
        "    for i in range(len(embedding)):\n",
        "        mlist = embedding[0][i]\n",
        "        embedding[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1\n",
        "\n",
        "    return torch.from_numpy(embedding).to(device)\n",
        "\n",
        "\n",
        "def recording(name):\n",
        "    # import sounddevice\n",
        "    # # from scipy.io.wavefile import write\n",
        "    # filename = name\n",
        "    # fps = 16000\n",
        "    # duration = 3\n",
        "    # print(\"Recording ..\")\n",
        "    # recording = sounddevice.rec(int(duration * fps), samplerate = fps, channels = 2)\n",
        "    # sounddevice.wait()\n",
        "    # print(\"Done.\")\n",
        "    # write(filename, fps, recording)\n",
        "    # return filename + \".wav\"\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCDZ2CSE4Mit"
      },
      "source": [
        "## START TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOYlypNI4RD2",
        "outputId": "f11fb1b7-cbdd-499c-a9d7-c779d74368a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Loss: 0.8832\n",
            "Epoch [2/300], Loss: 0.8972\n",
            "Epoch [3/300], Loss: 0.5650\n",
            "Epoch [4/300], Loss: 0.6794\n",
            "Epoch [5/300], Loss: 0.5243\n",
            "Epoch [6/300], Loss: 0.7353\n",
            "Epoch [7/300], Loss: 0.6823\n",
            "Epoch [8/300], Loss: 0.6112\n",
            "Epoch [9/300], Loss: 0.6755\n",
            "Epoch [10/300], Loss: 0.6557\n",
            "Epoch [11/300], Loss: 0.6486\n",
            "Epoch [12/300], Loss: 0.4886\n",
            "Epoch [13/300], Loss: 0.5273\n",
            "Epoch [14/300], Loss: 0.3690\n",
            "Epoch [15/300], Loss: 0.3835\n",
            "Epoch [16/300], Loss: 0.3074\n",
            "Epoch [17/300], Loss: 0.2416\n",
            "Epoch [18/300], Loss: 0.5341\n",
            "Epoch [19/300], Loss: 0.2849\n",
            "Epoch [20/300], Loss: 0.2842\n",
            "Epoch [21/300], Loss: 0.4155\n",
            "Epoch [22/300], Loss: 0.4673\n",
            "Epoch [23/300], Loss: 0.3647\n",
            "Epoch [24/300], Loss: 0.2760\n",
            "Epoch [25/300], Loss: 0.4207\n",
            "Epoch [26/300], Loss: 0.4262\n",
            "Epoch [27/300], Loss: 0.3051\n",
            "Epoch [28/300], Loss: 0.4757\n",
            "Epoch [29/300], Loss: 0.3892\n",
            "Epoch [30/300], Loss: 0.3233\n",
            "Epoch [31/300], Loss: 0.4391\n",
            "Epoch [32/300], Loss: 0.2594\n",
            "Epoch [33/300], Loss: 0.4119\n",
            "Epoch [34/300], Loss: 0.0720\n",
            "Epoch [35/300], Loss: 0.4572\n",
            "Epoch [36/300], Loss: 0.2634\n",
            "Epoch [37/300], Loss: 0.4299\n",
            "Epoch [38/300], Loss: 0.2894\n",
            "Epoch [39/300], Loss: 0.2321\n",
            "Epoch [40/300], Loss: 0.2272\n",
            "Epoch [41/300], Loss: 0.2995\n",
            "Epoch [42/300], Loss: 0.2793\n",
            "Epoch [43/300], Loss: 0.3025\n",
            "Epoch [44/300], Loss: 0.2906\n",
            "Epoch [45/300], Loss: 0.2776\n",
            "Epoch [46/300], Loss: 0.4672\n",
            "Epoch [47/300], Loss: 0.4696\n",
            "Epoch [48/300], Loss: 0.2541\n",
            "Epoch [49/300], Loss: 0.3415\n",
            "Epoch [50/300], Loss: 0.3473\n",
            "Epoch [51/300], Loss: 0.5867\n",
            "Epoch [52/300], Loss: 0.2789\n",
            "Epoch [53/300], Loss: 0.3321\n",
            "Epoch [54/300], Loss: 0.4540\n",
            "Epoch [55/300], Loss: 0.3556\n",
            "Epoch [56/300], Loss: 0.2969\n",
            "Epoch [57/300], Loss: 0.1514\n",
            "Epoch [58/300], Loss: 0.3377\n",
            "Epoch [59/300], Loss: 0.2466\n",
            "Epoch [60/300], Loss: 0.2005\n",
            "Epoch [61/300], Loss: 0.2875\n",
            "Epoch [62/300], Loss: 0.5390\n",
            "Epoch [63/300], Loss: 0.2822\n",
            "Epoch [64/300], Loss: 0.4235\n",
            "Epoch [65/300], Loss: 0.2905\n",
            "Epoch [66/300], Loss: 0.2660\n",
            "Epoch [67/300], Loss: 0.2827\n",
            "Epoch [68/300], Loss: 0.2431\n",
            "Epoch [69/300], Loss: 0.1479\n",
            "Epoch [70/300], Loss: 0.3209\n",
            "Epoch [71/300], Loss: 0.2800\n",
            "Epoch [72/300], Loss: 0.2687\n",
            "Epoch [73/300], Loss: 0.0769\n",
            "Epoch [74/300], Loss: 0.3061\n",
            "Epoch [75/300], Loss: 0.2775\n",
            "Epoch [76/300], Loss: 0.4945\n",
            "Epoch [77/300], Loss: 0.1748\n",
            "Epoch [78/300], Loss: 0.1306\n",
            "Epoch [79/300], Loss: 0.2676\n",
            "Epoch [80/300], Loss: 0.1498\n",
            "Epoch [81/300], Loss: 0.3141\n",
            "Epoch [82/300], Loss: 0.2466\n",
            "Epoch [83/300], Loss: 0.3521\n",
            "Epoch [84/300], Loss: 0.2854\n",
            "Epoch [85/300], Loss: 0.2825\n",
            "Epoch [86/300], Loss: 0.1724\n",
            "Epoch [87/300], Loss: 0.2267\n",
            "Epoch [88/300], Loss: 0.2136\n",
            "Epoch [89/300], Loss: 0.2575\n",
            "Epoch [90/300], Loss: 0.2533\n",
            "Epoch [91/300], Loss: 0.2565\n",
            "Epoch [92/300], Loss: 0.3152\n",
            "Epoch [93/300], Loss: 0.2459\n",
            "Epoch [94/300], Loss: 0.2227\n",
            "Epoch [95/300], Loss: 0.1780\n",
            "Epoch [96/300], Loss: 0.1947\n",
            "Epoch [97/300], Loss: 0.2534\n",
            "Epoch [98/300], Loss: 0.1907\n",
            "Epoch [99/300], Loss: 0.2618\n",
            "Epoch [100/300], Loss: 0.2039\n",
            "Epoch [101/300], Loss: 0.2315\n",
            "Epoch [102/300], Loss: 0.4342\n",
            "Epoch [103/300], Loss: 0.1032\n",
            "Epoch [104/300], Loss: 0.2841\n",
            "Epoch [105/300], Loss: 0.1398\n",
            "Epoch [106/300], Loss: 0.2963\n",
            "Epoch [107/300], Loss: 0.3718\n",
            "Epoch [108/300], Loss: 0.2128\n",
            "Epoch [109/300], Loss: 0.1005\n",
            "Epoch [110/300], Loss: 0.3387\n",
            "Epoch [111/300], Loss: 0.4601\n",
            "Epoch [112/300], Loss: 0.2438\n",
            "Epoch [113/300], Loss: 0.2036\n",
            "Epoch [114/300], Loss: 0.2209\n",
            "Epoch [115/300], Loss: 0.1544\n",
            "Epoch [116/300], Loss: 0.2141\n",
            "Epoch [117/300], Loss: 0.1081\n",
            "Epoch [118/300], Loss: 0.1855\n",
            "Epoch [119/300], Loss: 0.1817\n",
            "Epoch [120/300], Loss: 0.1679\n",
            "Epoch [121/300], Loss: 0.3207\n",
            "Epoch [122/300], Loss: 0.4497\n",
            "Epoch [123/300], Loss: 0.2002\n",
            "Epoch [124/300], Loss: 0.1768\n",
            "Epoch [125/300], Loss: 0.1786\n",
            "Epoch [126/300], Loss: 0.2686\n",
            "Epoch [127/300], Loss: 0.2107\n",
            "Epoch [128/300], Loss: 0.1752\n",
            "Epoch [129/300], Loss: 0.2987\n",
            "Epoch [130/300], Loss: 0.2443\n",
            "Epoch [131/300], Loss: 0.2762\n",
            "Epoch [132/300], Loss: 0.2984\n",
            "Epoch [133/300], Loss: 0.1719\n",
            "Epoch [134/300], Loss: 0.2160\n",
            "Epoch [135/300], Loss: 0.1322\n",
            "Epoch [136/300], Loss: 0.0650\n",
            "Epoch [137/300], Loss: 0.1197\n",
            "Epoch [138/300], Loss: 0.0535\n",
            "Epoch [139/300], Loss: 0.2542\n",
            "Epoch [140/300], Loss: 0.1072\n",
            "Epoch [141/300], Loss: 0.0787\n",
            "Epoch [142/300], Loss: 0.2539\n",
            "Epoch [143/300], Loss: 0.2597\n",
            "Epoch [144/300], Loss: 0.0870\n",
            "Epoch [145/300], Loss: 0.4432\n",
            "Epoch [146/300], Loss: 0.1166\n",
            "Epoch [147/300], Loss: 0.2094\n",
            "Epoch [148/300], Loss: 0.0824\n",
            "Epoch [149/300], Loss: 0.2069\n",
            "Epoch [150/300], Loss: 0.1573\n",
            "Epoch [151/300], Loss: 0.2152\n",
            "Epoch [152/300], Loss: 0.2216\n",
            "Epoch [153/300], Loss: 0.2487\n",
            "Epoch [154/300], Loss: 0.1726\n",
            "Epoch [155/300], Loss: 0.2866\n",
            "Epoch [156/300], Loss: 0.1886\n",
            "Epoch [157/300], Loss: 0.1950\n",
            "Epoch [158/300], Loss: 0.1491\n",
            "Epoch [159/300], Loss: 0.1107\n",
            "Epoch [160/300], Loss: 0.0563\n",
            "Epoch [161/300], Loss: 0.1274\n",
            "Epoch [162/300], Loss: 0.0879\n",
            "Epoch [163/300], Loss: 0.1234\n",
            "Epoch [164/300], Loss: 0.1153\n",
            "Epoch [165/300], Loss: 0.0464\n",
            "Epoch [166/300], Loss: 0.1452\n",
            "Epoch [167/300], Loss: 0.1299\n",
            "Epoch [168/300], Loss: 0.1203\n",
            "Epoch [169/300], Loss: 0.1456\n",
            "Epoch [170/300], Loss: 0.1723\n",
            "Epoch [171/300], Loss: 0.1561\n",
            "Epoch [172/300], Loss: 0.0558\n",
            "Epoch [173/300], Loss: 0.1538\n",
            "Epoch [174/300], Loss: 0.0812\n",
            "Epoch [175/300], Loss: 0.4063\n",
            "Epoch [176/300], Loss: 0.0535\n",
            "Epoch [177/300], Loss: 0.1455\n",
            "Epoch [178/300], Loss: 0.2122\n",
            "Epoch [179/300], Loss: 0.1348\n",
            "Epoch [180/300], Loss: 0.0863\n",
            "Epoch [181/300], Loss: 0.0979\n",
            "Epoch [182/300], Loss: 0.1467\n",
            "Epoch [183/300], Loss: 0.3054\n",
            "Epoch [184/300], Loss: 0.1772\n",
            "Epoch [185/300], Loss: 0.2164\n",
            "Epoch [186/300], Loss: 0.0144\n",
            "Epoch [187/300], Loss: 0.1668\n",
            "Epoch [188/300], Loss: 0.0656\n",
            "Epoch [189/300], Loss: 0.1765\n",
            "Epoch [190/300], Loss: 0.4305\n",
            "Epoch [191/300], Loss: 0.0806\n",
            "Epoch [192/300], Loss: 0.0982\n",
            "Epoch [193/300], Loss: 0.1412\n",
            "Epoch [194/300], Loss: 0.0501\n",
            "Epoch [195/300], Loss: 0.0577\n",
            "Epoch [196/300], Loss: 0.2045\n",
            "Epoch [197/300], Loss: 0.0742\n",
            "Epoch [198/300], Loss: 0.0334\n",
            "Epoch [199/300], Loss: 0.0862\n",
            "Epoch [200/300], Loss: 0.2822\n",
            "Epoch [201/300], Loss: 0.0444\n",
            "Epoch [202/300], Loss: 0.1314\n",
            "Epoch [203/300], Loss: 0.0347\n",
            "Epoch [204/300], Loss: 0.0091\n",
            "Epoch [205/300], Loss: 0.1922\n",
            "Epoch [207/300], Loss: 0.0349\n",
            "Epoch [208/300], Loss: 0.1382\n",
            "Epoch [209/300], Loss: 0.2388\n",
            "Epoch [210/300], Loss: 0.1253\n",
            "Epoch [211/300], Loss: 0.2361\n",
            "Epoch [212/300], Loss: 0.1058\n",
            "Epoch [213/300], Loss: 0.1100\n",
            "Epoch [214/300], Loss: 0.0633\n",
            "Epoch [215/300], Loss: 0.2819\n",
            "Epoch [216/300], Loss: 0.1627\n",
            "Epoch [217/300], Loss: 0.0226\n",
            "Epoch [218/300], Loss: 0.0770\n",
            "Epoch [219/300], Loss: 0.0512\n",
            "Epoch [220/300], Loss: 0.1142\n",
            "Epoch [221/300], Loss: 0.0599\n",
            "Epoch [222/300], Loss: 0.0492\n",
            "Epoch [223/300], Loss: 0.0228\n",
            "Epoch [224/300], Loss: 0.0451\n",
            "Epoch [225/300], Loss: 0.1791\n",
            "Epoch [226/300], Loss: 0.0635\n",
            "Epoch [227/300], Loss: 0.0656\n",
            "Epoch [228/300], Loss: 0.1005\n",
            "Epoch [229/300], Loss: 0.0215\n",
            "Epoch [230/300], Loss: 0.3249\n",
            "Epoch [231/300], Loss: 0.0110\n",
            "Epoch [232/300], Loss: 0.0592\n",
            "Epoch [233/300], Loss: 0.0719\n",
            "Epoch [234/300], Loss: 0.0051\n",
            "Epoch [235/300], Loss: 0.1866\n",
            "Epoch [236/300], Loss: 0.1027\n",
            "Epoch [237/300], Loss: 0.0203\n",
            "Epoch [238/300], Loss: 0.0097\n",
            "Epoch [239/300], Loss: 0.0004\n",
            "Epoch [240/300], Loss: 0.1638\n",
            "Epoch [241/300], Loss: 0.0119\n",
            "Epoch [242/300], Loss: 0.0255\n",
            "Epoch [243/300], Loss: 0.1276\n",
            "Epoch [244/300], Loss: 0.0900\n",
            "Epoch [245/300], Loss: 0.0328\n",
            "Epoch [246/300], Loss: 0.0593\n",
            "Epoch [247/300], Loss: 0.0455\n",
            "Epoch [248/300], Loss: 0.6127\n",
            "Epoch [249/300], Loss: 0.0097\n",
            "Epoch [250/300], Loss: 0.0469\n",
            "Epoch [251/300], Loss: 0.3331\n",
            "Epoch [252/300], Loss: 0.0623\n",
            "Epoch [253/300], Loss: 0.0243\n",
            "Epoch [254/300], Loss: 0.1252\n",
            "Epoch [255/300], Loss: 0.0275\n",
            "Epoch [256/300], Loss: 0.0855\n",
            "Epoch [257/300], Loss: 0.0281\n",
            "Epoch [258/300], Loss: 0.0603\n",
            "Epoch [259/300], Loss: 0.0173\n",
            "Epoch [260/300], Loss: 0.0765\n",
            "Epoch [261/300], Loss: 0.2843\n",
            "Epoch [262/300], Loss: 0.0533\n",
            "Epoch [263/300], Loss: 0.1395\n",
            "Epoch [264/300], Loss: 0.0842\n",
            "Epoch [265/300], Loss: 0.0161\n",
            "Epoch [266/300], Loss: 0.0641\n",
            "Epoch [267/300], Loss: 0.1539\n",
            "Epoch [268/300], Loss: 0.0041\n",
            "Epoch [269/300], Loss: 0.0820\n",
            "Epoch [270/300], Loss: 0.0326\n",
            "Epoch [271/300], Loss: 0.0701\n",
            "Epoch [272/300], Loss: 0.0500\n",
            "Epoch [273/300], Loss: 0.1704\n",
            "Epoch [274/300], Loss: 0.0816\n",
            "Epoch [275/300], Loss: 0.0239\n",
            "Epoch [276/300], Loss: 0.0333\n",
            "Epoch [277/300], Loss: 0.0146\n",
            "Epoch [278/300], Loss: 0.0570\n",
            "Epoch [279/300], Loss: 0.0114\n",
            "Epoch [280/300], Loss: 0.0441\n",
            "Epoch [281/300], Loss: 0.1983\n",
            "Epoch [282/300], Loss: 0.0306\n",
            "Epoch [283/300], Loss: 0.0902\n",
            "Epoch [284/300], Loss: 0.0330\n",
            "Epoch [285/300], Loss: 0.0322\n",
            "Epoch [286/300], Loss: 0.0062\n",
            "Epoch [287/300], Loss: 0.0046\n",
            "Epoch [288/300], Loss: 0.0737\n",
            "Epoch [289/300], Loss: 0.0594\n",
            "Epoch [290/300], Loss: 0.0249\n",
            "Epoch [291/300], Loss: 0.1207\n",
            "Epoch [292/300], Loss: 0.1148\n",
            "Epoch [293/300], Loss: 0.0366\n",
            "Epoch [294/300], Loss: 0.0107\n",
            "Epoch [295/300], Loss: 0.0048\n",
            "Epoch [296/300], Loss: 0.0183\n",
            "Epoch [297/300], Loss: 0.0048\n",
            "Epoch [298/300], Loss: 0.0050\n",
            "Epoch [299/300], Loss: 0.0135\n",
            "Epoch [300/300], Loss: 0.0738\n"
          ]
        }
      ],
      "source": [
        "aer_dataset = Data()\n",
        "cnn = ConvNet(3, aer_dataset)\n",
        "cnn.to(device)\n",
        "cnn.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn, \"/content/data/MyDrive/dl/model6.pth\")"
      ],
      "metadata": {
        "id": "JEZvqhOZbHFl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWNPkZ04UCp"
      },
      "source": [
        "## START TEST\n",
        "\n",
        "\n",
        "Accuracy of the network: 76.20192307692308 %\n",
        "\n",
        "Accuracy of positive: 65.60693641618496 %\n",
        "\n",
        "Accuracy of neutral: 71.14427860696517 %\n",
        "\n",
        "Accuracy of negative: 82.88159771754636 %\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2c1Cu8n2Wv",
        "outputId": "8d304503-2c45-4588-876b-e29131d58c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 76.20192307692308 %\n",
            "Accuracy of positive: 65.60693641618496 %\n",
            "Accuracy of neutral: 71.14427860696517 %\n",
            "Accuracy of negative: 82.88159771754636 %\n"
          ]
        }
      ],
      "source": [
        "test = TestConvNet(cnn, aer_dataset)\n",
        "n_class = test.test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display the confusion matrix as a heatmap\n",
        "arr = confusion_matrix(aer_dataset.test_y.detach().cpu().numpy(), n_class)\n",
        "class_names = ['Positive', 'Neutral', ' Negative']\n",
        "print(arr)\n",
        "df_cm = pd.DataFrame(arr, class_names, class_names)\n",
        "plt.figure(figsize = (9,6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "b3Ok6cs_J9K_",
        "outputId": "e130f6d9-4a57-4602-f3a2-1964a7cddd00"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[228  27  98]\n",
            " [ 25 140  40]\n",
            " [ 92  42 579]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awUU1AREn3fk"
      },
      "source": [
        "## INFERENCE\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-sKsHj4ZwS",
        "outputId": "cb54c7e4-08d0-4fb3-ea36-2b109c77b82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([3.6399611e-04, 4.7662212e-08, 9.9963593e-01], dtype=float32)]\n",
            "negative\n"
          ]
        }
      ],
      "source": [
        "inf_X = inference('/content/data/MyDrive/dl/angry2.wav')\n",
        "X = Norm(inf_X)\n",
        "y = cnn.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "predict = [np.exp(c) for c in y]\n",
        "max = np.argmax(predict)\n",
        "print(predict)\n",
        "print(classes[max])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "train_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}