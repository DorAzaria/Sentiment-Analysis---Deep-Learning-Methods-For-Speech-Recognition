{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Voice-Emotion-Recognition/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685hzZYY0Oua",
        "outputId": "6195305d-4974-464b-d610-97ac7adc29f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3h7Ml8w1u_g"
      },
      "source": [
        "# **IMPORTS**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCj-OhuD1uyS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import mat\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "!sudo apt-get install libportaudio2\n",
        "!sudo apt-get install python-scipy\n",
        "\n",
        "!pip install sounddevice\n",
        "!pip install scipy\n",
        "\n",
        "import sounddevice\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "classes = {0: 'positive', 1:'neutral', 2:'negative'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4QYGCi02Y7"
      },
      "source": [
        "# **PREPROCESS**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "820sVvYW1E5o"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        file_handler = open('/content/data/MyDrive/dl/dataset5.pth', 'rb')\n",
        "        self.data = pickle.load(file_handler)\n",
        "        x_dataset = [embedding[1] for embedding in self.data]\n",
        "        y_dataset = [label[2] for label in self.data]\n",
        "\n",
        "        #[70, 15, 15]\n",
        "        train_x, rem_x, train_y, rem_y = train_test_split(np.array(x_dataset), np.array(y_dataset), train_size=0.70) \n",
        "        valid_x, test_x, valid_y, test_y = train_test_split(rem_x, rem_y, test_size=0.5)\n",
        "\n",
        "        self.train_x = torch.from_numpy(train_x)\n",
        "        self.train_y = torch.from_numpy(train_y)\n",
        "        torch_train = TensorDataset(self.train_x, self.train_y)\n",
        "\n",
        "        self.valid_x = torch.from_numpy(valid_x)\n",
        "        self.valid_y = torch.from_numpy(valid_y)\n",
        "        torch_valid = TensorDataset(self.valid_x, self.valid_y)\n",
        "        \n",
        "        self.test_x = torch.from_numpy(test_x)\n",
        "        self.test_y = torch.from_numpy(test_y)\n",
        "        torch_test = TensorDataset(self.test_x, self.test_y)\n",
        "        \n",
        "        self.train_loader = DataLoader(torch_train, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.valid_loader = DataLoader(torch_valid, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.test_loader = DataLoader(torch_test, batch_size=32, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOT**"
      ],
      "metadata": {
        "id": "Ad32J_Fi1gK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_plot(train_acc, test_acc, train_loss):\n",
        "    epochs_x_axis = np.linspace(1, len(train_acc), len(train_acc)).astype(int)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "    axes[0].plot(epochs_x_axis, train_acc)\n",
        "    axes[0].plot(epochs_x_axis, test_acc)\n",
        "    axes[0].set_ylabel(\"Accuracy\")\n",
        "    axes[0].set_xlabel(\"epochs\")\n",
        "    axes[0].legend(['train', 'test'])\n",
        "    \n",
        "\n",
        "    axes[1].plot(epochs_x_axis, train_loss)\n",
        "    axes[1].set_ylabel(\"Training loss\")\n",
        "    axes[1].set_xlabel(\"epochs\")\n",
        "    fig.tight_layout()"
      ],
      "metadata": {
        "id": "0bAutrVXMY1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotEarlyStopping(train_loss, valid_loss):\n",
        "  epochs_x_axis = np.linspace(1, len(train_loss), len(train_loss)).astype(int)\n",
        "  plt.plot(epochs_x_axis, train_loss)\n",
        "  plt.plot(epochs_x_axis, valid_loss)\n",
        "  plt.title('Train VS Valid Loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train loss', 'validation loss'], loc='upper left')"
      ],
      "metadata": {
        "id": "ZxC8IagDCrDZ"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9NoyVXt2F_6"
      },
      "source": [
        "# **Model**\n",
        "\n",
        "1, 149, 32\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "56-6NfI62iiW"
      },
      "outputs": [],
      "source": [
        "DROP_OUT = 0.8\n",
        "NUM_OF_CLASSES = 3\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.epochs = 300\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # Model Architecture\n",
        "        self.first_conv = nn.Conv2d(1, 96, kernel_size=(5, 5), padding=1) # (96, 147, 30)\n",
        "        self.first_bn = nn.BatchNorm2d(96)\n",
        "        self.first_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2)) # (96, 73, 14)\n",
        "\n",
        "        self.second_conv = nn.Conv2d(96, 256, kernel_size=(5, 5), padding=1) # (256, 71, 12)\n",
        "        self.second_bn = nn.BatchNorm2d(256)\n",
        "        self.second_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1)) # (256, 69, 10)\n",
        "\n",
        "        self.third_conv = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=1) # (384, 69, 10 )\n",
        "        self.third_bn = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.forth_conv = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.forth_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fifth_conv = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.fifth_bn = nn.BatchNorm2d(256)\n",
        "        self.fifth_polling = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1)) # (256, 68, 9)\n",
        "\n",
        "        self.sixth_conv = nn.Conv2d(256, 64, kernel_size=(2, 2), padding=1) # (64, 69, 10)\n",
        "\n",
        "        self.seventh_conv = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1) # (64, 69, 10)\n",
        "        self.seventh_polling = nn.MaxPool2d(kernel_size=(2,2), stride=(2, 2)) # (64, 34, 5)\n",
        "\n",
        "        self.eighth_conv = nn.Conv2d(64, 32, kernel_size=(3,3), padding=1) # (32, 34, 5)\n",
        "        self.first_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.avg_polling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.first_dense = nn.Linear(32, 1024)\n",
        "        self.second_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.second_dense = nn.Linear(1024, NUM_OF_CLASSES)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = nn.ReLU()(self.first_conv(X))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.first_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.second_conv(x))\n",
        "        x = self.second_bn(x)\n",
        "        x = self.second_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.third_conv(x))\n",
        "        x = self.third_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.forth_conv(x))\n",
        "        x = self.forth_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.fifth_conv(x))\n",
        "        x = self.fifth_bn(x)\n",
        "        x = self.fifth_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.sixth_conv(x))\n",
        "\n",
        "        x = nn.ReLU()(self.seventh_conv(x))\n",
        "        x = self.seventh_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.eighth_conv(x))\n",
        "\n",
        "        x = self.first_drop(x)\n",
        "        x = self.avg_polling(x)\n",
        "\n",
        "        x = x.view(-1, x.shape[1])  # output channel for flatten before entering the dense layer\n",
        "\n",
        "        x = nn.ReLU()(self.first_dense(x))\n",
        "        x = self.second_drop(x)\n",
        "\n",
        "        x = self.second_dense(x)\n",
        "        y = nn.LogSoftmax(dim=1)(x)  # consider using Log-Softmax\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_epochs(self):\n",
        "        return self.epochs\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.learning_rate\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "    classes = torch.argmax(predictions, dim=1)\n",
        "    return torch.mean((classes == labels).float()).to('cpu')"
      ],
      "metadata": {
        "id": "jQ36CsINjRlf"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpaZ7PlR3CRK"
      },
      "source": [
        "# **TEST**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "oO0UNKVK2-3S"
      },
      "outputs": [],
      "source": [
        "def test(convnet_model, dataset):\n",
        "    results = []\n",
        "    test_batch_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        n_class_correct = [0 for i in range(3)]\n",
        "        n_class_samples = [0 for i in range(3)]\n",
        "        \n",
        "        n_class = [0 for i in range(len(dataset.test_y))]\n",
        "        j = 0\n",
        "        \n",
        "        for embedding, labels in dataset.test_loader:\n",
        "            \n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "            outputs = convnet_model(embedding)\n",
        "\n",
        "            # max returns (value ,index)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            test_batch_acc.append(accuracy(outputs, labels))\n",
        "\n",
        "            for i in range(convnet_model.batch_size):\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if label == pred:\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "                n_class[j] = pred.view(-1).detach().cpu().numpy()[0]\n",
        "                j += 1\n",
        "        returned_acc = sum(test_batch_acc)/len(test_batch_acc)\n",
        "\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        results.append(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "        for i in range(3):\n",
        "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "            results.append(f'Accuracy of {classes[i]}: {acc} %')\n",
        "        \n",
        "        return n_class, results, returned_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tq7sCSuj_I8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(convnet_model, dataset, loss_function):\n",
        "    # Settings\n",
        "    model.eval()\n",
        "    loss_total = 0\n",
        "\n",
        "    # Test validation data\n",
        "    with torch.no_grad():\n",
        "        for embedding, labels in dataset.valid_loader:\n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "\n",
        "            outputs = convnet_model(embedding)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_total += loss.item()\n",
        "\n",
        "    return loss_total / len(dataset.valid_loader)\n"
      ],
      "metadata": {
        "id": "z2IV8w_o_Iu8"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**\n",
        "---\n"
      ],
      "metadata": {
        "id": "Fmjphp7Uc8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(convnet_model, dataset):\n",
        "    save_counter=0\n",
        "    acc_vals_train = []\n",
        "    loss_vals_train = []\n",
        "    loss_valid = []\n",
        "    acc_vals_test = []\n",
        "    optimizer = torch.optim.Adam(convnet_model.parameters(), lr=convnet_model.learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    # [3304, 2895, 9004] --> 15k examples\n",
        "    # [1184, 688, 2363] --> 4235 examples\n",
        "    # criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([2.72518159806, 3.11018998273, 1])).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.99, 4, 1])).to(device)\n",
        "    \n",
        "    n_total_steps = len(dataset.train_loader)\n",
        "\n",
        "    # Early stopping\n",
        "    the_last_loss = 100\n",
        "    patience = 2\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(convnet_model.get_epochs()):\n",
        "\n",
        "        convnet_model.train()\n",
        "        epoch_acc = []\n",
        "        epoch_loss = []\n",
        "\n",
        "        for i, (embedding, labels) in enumerate(dataset.train_loader):\n",
        "\n",
        "            embedding = embedding.type(torch.FloatTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "        \n",
        "            labels = labels.to(device)\n",
        "            embedding = embedding.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = convnet_model.forward(embedding)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_acc.append(accuracy(outputs, labels))\n",
        "            epoch_loss.append(loss.item())\n",
        "            if i == 90:\n",
        "                print(f'Epoch [{epoch + 1}/{convnet_model.epochs}], Loss: {loss.item():.4f}')\n",
        "                \n",
        "        acc_vals_train.append(sum(epoch_acc)/len(epoch_acc))\n",
        "        loss_vals_train.append(sum(epoch_loss)/len(epoch_loss))\n",
        "\n",
        "        _ , _, test_acc = test(convnet_model, dataset)\n",
        "        acc_vals_test.append(test_acc)\n",
        "\n",
        "        #### EARLY STOPPING ~~~~~~~~~~~~~~~~~~~\n",
        "        the_current_loss = validation(convnet_model, dataset, criterion)\n",
        "        loss_valid.append(the_current_loss)\n",
        "\n",
        "        if the_current_loss > the_last_loss:\n",
        "            trigger_times += 1\n",
        "            print('trigger times:', trigger_times)\n",
        "            if loss_vals_train[-1] < 0.5:\n",
        "              torch.save(cnn, f\"/content/model{save_counter}.pth\")\n",
        "              print(f'saved model{save_counter}.pth')\n",
        "              save_counter += 1\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            trigger_times = 0\n",
        "\n",
        "        the_last_loss = the_current_loss\n",
        "        ##### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n",
        "    # my_plot(acc_vals_train, acc_vals_test, loss_vals_train)\n",
        "    plotEarlyStopping(loss_vals_train,loss_valid)"
      ],
      "metadata": {
        "id": "PasvGTEAc5AE"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdGGMiq35tS"
      },
      "source": [
        "# **Main**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQMhz2iXxNQ"
      },
      "source": [
        "## NORM AND INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "sj2SqEan3-Gz"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "\n",
        "\n",
        "def inference(file_name):\n",
        "    SAMPLE_RATE = 16000\n",
        "    waveform, sample_rate = torchaudio.load(filepath=file_name,  num_frames=SAMPLE_RATE * 3)\n",
        "    waveform = waveform.view(1, 96000)\n",
        "    waveform = waveform.to(device)\n",
        "    \n",
        "    if (len(waveform[0]) < 48000):\n",
        "        print(f'less than 3 seconds: {file_name}')\n",
        "\n",
        "    if sample_rate != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        embedding, _ = model(waveform)\n",
        "\n",
        "    return embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "def Norm(X):\n",
        "    embedding = X.detach().cpu().numpy()\n",
        "    for i in range(len(embedding)):\n",
        "        mlist = embedding[0][i]\n",
        "        embedding[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1\n",
        "\n",
        "    return torch.from_numpy(embedding).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCDZ2CSE4Mit"
      },
      "source": [
        "## START TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOYlypNI4RD2"
      },
      "outputs": [],
      "source": [
        "dataset = Data()\n",
        "cnn = ConvNet()\n",
        "cnn.to(device)\n",
        "train_model(cnn, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn, \"/content/data/MyDrive/dl/model206.pth\")"
      ],
      "metadata": {
        "id": "JEZvqhOZbHFl"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_import = torch.load(\"/content/data/MyDrive/dl/model202.pth\")"
      ],
      "metadata": {
        "id": "mrUVc4DLlOTz"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWNPkZ04UCp"
      },
      "source": [
        "## START TEST\n",
        "\n",
        "\n",
        "Accuracy of the network: 76.20192307692308 %\n",
        "\n",
        "Accuracy of positive: 65.60693641618496 %\n",
        "\n",
        "Accuracy of neutral: 71.14427860696517 %\n",
        "\n",
        "Accuracy of negative: 82.88159771754636 %\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2c1Cu8n2Wv",
        "outputId": "a2901d54-bc79-41e0-ed9d-0f1cabd258a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 77.63157894736842 %\n",
            "Accuracy of positive: 76.36363636363636 %\n",
            "Accuracy of neutral: 91.75257731958763 %\n",
            "Accuracy of negative: 74.27745664739885 %\n"
          ]
        }
      ],
      "source": [
        "n_class, results,  _ = test(model_import, dataset)\n",
        "\n",
        "for text in results:\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display the confusion matrix as a heatmap\n",
        "arr = confusion_matrix(dataset.test_y.detach().cpu().numpy(), n_class)\n",
        "class_names = ['Positive', 'Neutral', ' Negative']\n",
        "print(arr)\n",
        "df_cm = pd.DataFrame(arr, class_names, class_names)\n",
        "plt.figure(figsize = (9,6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "b3Ok6cs_J9K_",
        "outputId": "e2c93c7d-98be-44a2-9b65-e324e8331253"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[138  31   8]\n",
            " [  7  89   4]\n",
            " [ 50  52 257]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awUU1AREn3fk"
      },
      "source": [
        "## INFERENCE\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(y):\n",
        "  predict = [np.exp(c) for c in y]\n",
        "  max = np.argmax(predict)\n",
        "  print(f'Predicted: {classes[max].capitalize()}')\n",
        "  print(f'Positive: {round(predict[0][0]*100, 4)}%')\n",
        "  print(f'Neutral: {round(predict[0][1]*100, 4)}%')\n",
        "  print(f'Negative: {round(predict[0][2]*100, 4)}%')"
      ],
      "metadata": {
        "id": "Ph6iOQImBFJF"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)."
      ],
      "metadata": {
        "id": "n9w_o2lLtPYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-sKsHj4ZwS",
        "outputId": "6ebb83a3-d228-48fd-f366-d32a62b652a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Negative\n",
            "Positive: 1.0791%\n",
            "Neutral: 0.0033%\n",
            "Negative: 98.9176%\n"
          ]
        }
      ],
      "source": [
        "inf_X = inference('/content/data/MyDrive/dl/EMOVO/negative/dis-m2-l3.wav')\n",
        "X = Norm(inf_X)\n",
        "y = model_import.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "print_results(y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "train_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}