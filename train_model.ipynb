{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorAzaria/Sentiment-Analysis-Deep-Learning-Methods-For-Speech-Recognition/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685hzZYY0Oua",
        "outputId": "faed7661-cb80-4e96-9d90-17720b6582e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3h7Ml8w1u_g"
      },
      "source": [
        "# **IMPORTS**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCj-OhuD1uyS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import torchaudio\n",
        "from numpy import mat\n",
        "\n",
        "!sudo apt-get install libportaudio2\n",
        "!sudo apt-get install python-scipy\n",
        "\n",
        "!pip install sounddevice\n",
        "!pip install scipy\n",
        "\n",
        "import sounddevice\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS4QYGCi02Y7"
      },
      "source": [
        "# **PREPROCESS**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "820sVvYW1E5o"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        file_handler = open('/content/data/MyDrive/dl/dataset.pth', 'rb')\n",
        "        data = pickle.load(file_handler)\n",
        "        x_dataset = [embedding[1] for embedding in data]\n",
        "        y_dataset = [label[2] for label in data]\n",
        "        train_x, test_x, train_y, test_y = train_test_split(np.array(x_dataset), np.array(y_dataset), test_size=0.30)\n",
        "        self.train_x = torch.from_numpy(train_x)\n",
        "        self.train_y = torch.from_numpy(train_y)\n",
        "        torch_train = TensorDataset(self.train_x, self.train_y)\n",
        "        \n",
        "        self.test_x = torch.from_numpy(test_x)\n",
        "        self.test_y = torch.from_numpy(test_y)\n",
        "        torch_test = TensorDataset(self.test_x, self.test_y)\n",
        "        \n",
        "\n",
        "        self.train_loader = DataLoader(torch_train, batch_size=32, drop_last=True, shuffle=True)\n",
        "        self.test_loader = DataLoader(torch_test, batch_size=32, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9NoyVXt2F_6"
      },
      "source": [
        "# **TRAIN**\n",
        "\n",
        "1, 149, 32\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "56-6NfI62iiW"
      },
      "outputs": [],
      "source": [
        "DROP_OUT = 0.5\n",
        "NUM_OF_CLASSES = 3\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_of_classes, dataset):\n",
        "        super().__init__()\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.epochs = 300\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 0.001\n",
        "        self.dataset = dataset\n",
        "\n",
        "        # Model Architecture\n",
        "        self.first_conv = nn.Conv2d(1, 96, kernel_size=(5, 5), padding=1) # (96, 147, 30)\n",
        "        self.first_bn = nn.BatchNorm2d(96)\n",
        "        self.first_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2)) # (96, 73, 14)\n",
        "\n",
        "        self.second_conv = nn.Conv2d(96, 256, kernel_size=(5, 5), padding=1) # (256, 71, 12)\n",
        "        self.second_bn = nn.BatchNorm2d(256)\n",
        "        self.second_polling = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1)) # (256, 69, 10)\n",
        "\n",
        "        self.third_conv = nn.Conv2d(256, 384, kernel_size=(3, 3), padding=1) # (384, 69, 10 )\n",
        "        self.third_bn = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.forth_conv = nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.forth_bn = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fifth_conv = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1) # (256, 69, 10)\n",
        "        self.fifth_bn = nn.BatchNorm2d(256)\n",
        "        self.fifth_polling = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1)) # (256, 68, 9)\n",
        "\n",
        "        self.sixth_conv = nn.Conv2d(256, 64, kernel_size=(2, 2), padding=1) # (64, 69, 10)\n",
        "\n",
        "        self.seventh_conv = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1) # (64, 69, 10)\n",
        "        self.seventh_polling = nn.MaxPool2d(kernel_size=(2,2), stride=(2, 2)) # (64, 34, 5)\n",
        "\n",
        "        self.eighth_conv = nn.Conv2d(64, 32, kernel_size=(3,3), padding=1) # (32, 34, 5)\n",
        "        self.first_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.avg_polling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.first_dense = nn.Linear(32, 1024)\n",
        "        self.second_drop = nn.Dropout(p=DROP_OUT)\n",
        "\n",
        "        self.second_dense = nn.Linear(1024, num_of_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = nn.ReLU()(self.first_conv(X))\n",
        "        x = self.first_bn(x)\n",
        "        x = self.first_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.second_conv(x))\n",
        "        x = self.second_bn(x)\n",
        "        x = self.second_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.third_conv(x))\n",
        "        x = self.third_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.forth_conv(x))\n",
        "        x = self.forth_bn(x)\n",
        "\n",
        "        x = nn.ReLU()(self.fifth_conv(x))\n",
        "        x = self.fifth_bn(x)\n",
        "        x = self.fifth_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.sixth_conv(x))\n",
        "\n",
        "        x = nn.ReLU()(self.seventh_conv(x))\n",
        "        x = self.seventh_polling(x)\n",
        "\n",
        "        x = nn.ReLU()(self.eighth_conv(x))\n",
        "\n",
        "        x = self.first_drop(x)\n",
        "        x = self.avg_polling(x)\n",
        "\n",
        "        x = x.view(-1, x.shape[1])  # output channel for flatten before entering the dense layer\n",
        "\n",
        "        x = nn.ReLU()(self.first_dense(x))\n",
        "        x = self.second_drop(x)\n",
        "\n",
        "        x = self.second_dense(x)\n",
        "        y = nn.LogSoftmax(dim=1)(x)  # consider using Log-Softmax\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_epochs(self):\n",
        "        return self.epochs\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.learning_rate\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return self.batch_size\n",
        "\n",
        "    def train_model(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([2.103336921, 3.203278689, 1])).to(device)\n",
        "\n",
        "        n_total_steps = len(self.dataset.train_loader)\n",
        "\n",
        "        for epoch in range(self.get_epochs()):\n",
        "            for i, (embedding, labels) in enumerate(self.dataset.train_loader):\n",
        "\n",
        "                embedding = embedding.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "            \n",
        "                labels = labels.to(device)\n",
        "                embedding = embedding.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.forward(embedding)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                if i == 74:\n",
        "                    print(f'Epoch [{epoch + 1}/{self.epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpaZ7PlR3CRK"
      },
      "source": [
        "# **TEST**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "oO0UNKVK2-3S"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classes = {0: 'positive', 1:'neutral', 2:'negative'}\n",
        "\n",
        "class TestConvNet:\n",
        "    def __init__(self, model, dataset):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.results = []\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_samples = 0\n",
        "            n_class_correct = [0 for i in range(3)]\n",
        "            n_class_samples = [0 for i in range(3)]\n",
        "            \n",
        "            n_class = [0 for i in range(len(self.dataset.test_y))]\n",
        "            j = 0\n",
        "            \n",
        "            \n",
        "            for embedding, labels in self.dataset.test_loader:\n",
        "                \n",
        "                embedding = embedding.type(torch.FloatTensor)\n",
        "                labels = labels.type(torch.LongTensor)\n",
        "                labels = labels.to(device)\n",
        "                embedding = embedding.to(device)\n",
        "                outputs = self.model(embedding)\n",
        "\n",
        "                # max returns (value ,index)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                n_samples += labels.size(0)\n",
        "                n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                for i in range(self.model.batch_size):\n",
        "                    label = labels[i]\n",
        "                    pred = predicted[i]\n",
        "                    if label == pred:\n",
        "                        n_class_correct[label] += 1\n",
        "                    n_class_samples[label] += 1\n",
        "                    n_class[j] = pred.view(-1).detach().cpu().numpy()[0]\n",
        "                    j += 1\n",
        "\n",
        "            acc = 100.0 * n_correct / n_samples\n",
        "            print(f'Accuracy of the network: {acc} %')\n",
        "            self.results.append(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "            for i in range(3):\n",
        "                acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "                print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "                self.results.append(f'Accuracy of {classes[i]}: {acc} %')\n",
        "            \n",
        "            return n_class\n",
        "        # saved_time = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
        "        # file_name = 'result.txt'\n",
        "        # directory = '/content/data/' + str(saved_time)\n",
        "        # os.mkdir(directory)\n",
        "\n",
        "        # with open(directory + \"/\" + file_name, 'w') as f:\n",
        "        #     for line in self.results:\n",
        "        #         f.write(line)\n",
        "        #         f.write('\\n')\n",
        "\n",
        "        # torch.save(self.model, directory + \"/model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdGGMiq35tS"
      },
      "source": [
        "# **Main**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQMhz2iXxNQ"
      },
      "source": [
        "## NORM AND INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sj2SqEan3-Gz"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "\n",
        "\n",
        "def inference(file_name):\n",
        "    SAMPLE_RATE = 16000\n",
        "    waveform, sample_rate = torchaudio.load(filepath=file_name,  num_frames=SAMPLE_RATE * 3)\n",
        "    waveform = waveform.view(1, 96000)\n",
        "    waveform = waveform.to(device)\n",
        "    \n",
        "    if (len(waveform[0]) < 48000):\n",
        "        print(f'less than 3 seconds: {file_name}')\n",
        "\n",
        "    if sample_rate != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        embedding, _ = model(waveform)\n",
        "\n",
        "    return embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "def Norm(X):\n",
        "    embedding = X.detach().cpu().numpy()\n",
        "    for i in range(len(embedding)):\n",
        "        mlist = embedding[0][i]\n",
        "        embedding[0][i] = 2 * (mlist - np.max(mlist)) / (np.max(mlist) - np.min(mlist)) + 1\n",
        "\n",
        "    return torch.from_numpy(embedding).to(device)\n",
        "\n",
        "\n",
        "def recording(name):\n",
        "    # import sounddevice\n",
        "    # # from scipy.io.wavefile import write\n",
        "    # filename = name\n",
        "    # fps = 16000\n",
        "    # duration = 3\n",
        "    # print(\"Recording ..\")\n",
        "    # recording = sounddevice.rec(int(duration * fps), samplerate = fps, channels = 2)\n",
        "    # sounddevice.wait()\n",
        "    # print(\"Done.\")\n",
        "    # write(filename, fps, recording)\n",
        "    # return filename + \".wav\"\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCDZ2CSE4Mit"
      },
      "source": [
        "## START TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOYlypNI4RD2",
        "outputId": "1107b4e7-c37e-49e1-cd0b-3d1dfae8daed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Loss: 1.2994\n",
            "Epoch [2/300], Loss: 1.2210\n",
            "Epoch [3/300], Loss: 1.1362\n",
            "Epoch [4/300], Loss: 1.0502\n",
            "Epoch [5/300], Loss: 1.0413\n",
            "Epoch [6/300], Loss: 1.0513\n",
            "Epoch [7/300], Loss: 0.9070\n",
            "Epoch [8/300], Loss: 1.0094\n",
            "Epoch [9/300], Loss: 0.8642\n",
            "Epoch [10/300], Loss: 0.8489\n",
            "Epoch [11/300], Loss: 1.0507\n",
            "Epoch [12/300], Loss: 1.1379\n",
            "Epoch [13/300], Loss: 0.9117\n",
            "Epoch [14/300], Loss: 1.1008\n",
            "Epoch [15/300], Loss: 0.9644\n",
            "Epoch [16/300], Loss: 0.8843\n",
            "Epoch [17/300], Loss: 0.9975\n",
            "Epoch [18/300], Loss: 0.8100\n",
            "Epoch [19/300], Loss: 0.9971\n",
            "Epoch [20/300], Loss: 1.0202\n",
            "Epoch [21/300], Loss: 1.0741\n",
            "Epoch [22/300], Loss: 0.6906\n",
            "Epoch [23/300], Loss: 0.8557\n",
            "Epoch [24/300], Loss: 0.7038\n",
            "Epoch [25/300], Loss: 0.5552\n",
            "Epoch [26/300], Loss: 1.1137\n",
            "Epoch [27/300], Loss: 0.6901\n",
            "Epoch [28/300], Loss: 0.7214\n",
            "Epoch [29/300], Loss: 0.8721\n",
            "Epoch [30/300], Loss: 0.4988\n",
            "Epoch [31/300], Loss: 0.5946\n",
            "Epoch [32/300], Loss: 0.6310\n",
            "Epoch [33/300], Loss: 0.6622\n",
            "Epoch [34/300], Loss: 0.5470\n",
            "Epoch [35/300], Loss: 0.5381\n",
            "Epoch [36/300], Loss: 0.6147\n",
            "Epoch [37/300], Loss: 0.7189\n",
            "Epoch [38/300], Loss: 0.6255\n",
            "Epoch [39/300], Loss: 0.6967\n",
            "Epoch [40/300], Loss: 0.6250\n",
            "Epoch [41/300], Loss: 0.5821\n",
            "Epoch [42/300], Loss: 0.6973\n",
            "Epoch [43/300], Loss: 0.4713\n",
            "Epoch [44/300], Loss: 0.4533\n",
            "Epoch [45/300], Loss: 0.4893\n",
            "Epoch [46/300], Loss: 0.5616\n",
            "Epoch [47/300], Loss: 0.4105\n",
            "Epoch [48/300], Loss: 0.3649\n",
            "Epoch [49/300], Loss: 0.9224\n",
            "Epoch [50/300], Loss: 0.4983\n",
            "Epoch [51/300], Loss: 0.4937\n",
            "Epoch [52/300], Loss: 0.5210\n",
            "Epoch [53/300], Loss: 0.7668\n",
            "Epoch [54/300], Loss: 0.7160\n",
            "Epoch [55/300], Loss: 0.7526\n",
            "Epoch [56/300], Loss: 0.4884\n",
            "Epoch [57/300], Loss: 0.4609\n",
            "Epoch [58/300], Loss: 0.3877\n",
            "Epoch [59/300], Loss: 0.5745\n",
            "Epoch [60/300], Loss: 0.5123\n",
            "Epoch [61/300], Loss: 0.3120\n",
            "Epoch [62/300], Loss: 0.3125\n",
            "Epoch [63/300], Loss: 0.5125\n",
            "Epoch [64/300], Loss: 0.3386\n",
            "Epoch [65/300], Loss: 0.4830\n",
            "Epoch [66/300], Loss: 0.3003\n",
            "Epoch [67/300], Loss: 0.5328\n",
            "Epoch [68/300], Loss: 0.3177\n",
            "Epoch [69/300], Loss: 0.3683\n",
            "Epoch [70/300], Loss: 0.6548\n",
            "Epoch [71/300], Loss: 0.3670\n",
            "Epoch [72/300], Loss: 0.2156\n",
            "Epoch [73/300], Loss: 0.2900\n",
            "Epoch [74/300], Loss: 0.4907\n",
            "Epoch [75/300], Loss: 0.3653\n",
            "Epoch [76/300], Loss: 0.3027\n",
            "Epoch [77/300], Loss: 0.2984\n",
            "Epoch [78/300], Loss: 0.3633\n",
            "Epoch [79/300], Loss: 0.4564\n",
            "Epoch [80/300], Loss: 0.2269\n",
            "Epoch [81/300], Loss: 0.3645\n",
            "Epoch [82/300], Loss: 0.2098\n",
            "Epoch [83/300], Loss: 0.5774\n",
            "Epoch [84/300], Loss: 0.3917\n",
            "Epoch [85/300], Loss: 0.5729\n",
            "Epoch [86/300], Loss: 0.5313\n",
            "Epoch [87/300], Loss: 0.2418\n",
            "Epoch [88/300], Loss: 0.5335\n",
            "Epoch [89/300], Loss: 0.3566\n",
            "Epoch [90/300], Loss: 0.2161\n",
            "Epoch [91/300], Loss: 0.2123\n",
            "Epoch [92/300], Loss: 0.7025\n",
            "Epoch [93/300], Loss: 0.3461\n",
            "Epoch [94/300], Loss: 0.2134\n",
            "Epoch [95/300], Loss: 0.4230\n",
            "Epoch [96/300], Loss: 0.3950\n",
            "Epoch [97/300], Loss: 0.2365\n",
            "Epoch [98/300], Loss: 0.2295\n",
            "Epoch [99/300], Loss: 0.3130\n",
            "Epoch [100/300], Loss: 0.6075\n",
            "Epoch [101/300], Loss: 0.2309\n",
            "Epoch [102/300], Loss: 0.2642\n",
            "Epoch [103/300], Loss: 0.3176\n",
            "Epoch [104/300], Loss: 0.1487\n",
            "Epoch [105/300], Loss: 0.3283\n",
            "Epoch [106/300], Loss: 0.3705\n",
            "Epoch [107/300], Loss: 0.1268\n",
            "Epoch [108/300], Loss: 0.3363\n",
            "Epoch [109/300], Loss: 0.4392\n",
            "Epoch [110/300], Loss: 0.2894\n",
            "Epoch [111/300], Loss: 0.1840\n",
            "Epoch [112/300], Loss: 0.5586\n",
            "Epoch [113/300], Loss: 0.2481\n",
            "Epoch [114/300], Loss: 0.2205\n",
            "Epoch [115/300], Loss: 0.1581\n",
            "Epoch [116/300], Loss: 0.4332\n",
            "Epoch [117/300], Loss: 0.4578\n",
            "Epoch [118/300], Loss: 0.1650\n",
            "Epoch [119/300], Loss: 0.3068\n",
            "Epoch [120/300], Loss: 0.5109\n",
            "Epoch [121/300], Loss: 0.3261\n",
            "Epoch [122/300], Loss: 0.0984\n",
            "Epoch [123/300], Loss: 0.3860\n",
            "Epoch [124/300], Loss: 0.1709\n",
            "Epoch [125/300], Loss: 0.2384\n",
            "Epoch [126/300], Loss: 0.0900\n",
            "Epoch [127/300], Loss: 0.3246\n",
            "Epoch [128/300], Loss: 0.4315\n",
            "Epoch [129/300], Loss: 0.2194\n",
            "Epoch [130/300], Loss: 0.2356\n",
            "Epoch [131/300], Loss: 0.2529\n",
            "Epoch [132/300], Loss: 0.2827\n",
            "Epoch [133/300], Loss: 0.3522\n",
            "Epoch [134/300], Loss: 0.3302\n",
            "Epoch [135/300], Loss: 0.1338\n",
            "Epoch [136/300], Loss: 0.3906\n",
            "Epoch [137/300], Loss: 0.2697\n",
            "Epoch [138/300], Loss: 0.2275\n",
            "Epoch [139/300], Loss: 0.2627\n",
            "Epoch [140/300], Loss: 0.1483\n",
            "Epoch [141/300], Loss: 0.3358\n",
            "Epoch [142/300], Loss: 0.4259\n",
            "Epoch [143/300], Loss: 0.2560\n",
            "Epoch [144/300], Loss: 0.2204\n",
            "Epoch [145/300], Loss: 0.2400\n",
            "Epoch [146/300], Loss: 0.2503\n",
            "Epoch [147/300], Loss: 0.1812\n",
            "Epoch [148/300], Loss: 0.3683\n",
            "Epoch [149/300], Loss: 0.0586\n",
            "Epoch [150/300], Loss: 0.0887\n",
            "Epoch [151/300], Loss: 0.2221\n",
            "Epoch [152/300], Loss: 0.1022\n",
            "Epoch [153/300], Loss: 0.4252\n",
            "Epoch [154/300], Loss: 0.1983\n",
            "Epoch [155/300], Loss: 0.3291\n",
            "Epoch [156/300], Loss: 0.3207\n",
            "Epoch [157/300], Loss: 0.2701\n",
            "Epoch [158/300], Loss: 0.2015\n",
            "Epoch [159/300], Loss: 0.1402\n",
            "Epoch [160/300], Loss: 0.2601\n",
            "Epoch [161/300], Loss: 0.0995\n",
            "Epoch [162/300], Loss: 0.1258\n",
            "Epoch [163/300], Loss: 0.2073\n",
            "Epoch [164/300], Loss: 0.1276\n",
            "Epoch [165/300], Loss: 0.1566\n",
            "Epoch [166/300], Loss: 0.3976\n",
            "Epoch [167/300], Loss: 0.6224\n",
            "Epoch [168/300], Loss: 0.2491\n",
            "Epoch [169/300], Loss: 0.1285\n",
            "Epoch [170/300], Loss: 0.1804\n",
            "Epoch [171/300], Loss: 0.1172\n",
            "Epoch [172/300], Loss: 0.1384\n",
            "Epoch [173/300], Loss: 0.0952\n",
            "Epoch [174/300], Loss: 0.3854\n",
            "Epoch [175/300], Loss: 0.2529\n",
            "Epoch [176/300], Loss: 0.2162\n",
            "Epoch [177/300], Loss: 0.2274\n",
            "Epoch [178/300], Loss: 0.0741\n",
            "Epoch [179/300], Loss: 0.1791\n",
            "Epoch [180/300], Loss: 0.1726\n",
            "Epoch [181/300], Loss: 0.2030\n",
            "Epoch [182/300], Loss: 0.1410\n",
            "Epoch [183/300], Loss: 0.1346\n",
            "Epoch [184/300], Loss: 0.1365\n",
            "Epoch [185/300], Loss: 0.0601\n",
            "Epoch [186/300], Loss: 0.2265\n",
            "Epoch [187/300], Loss: 0.1690\n",
            "Epoch [188/300], Loss: 0.1833\n",
            "Epoch [189/300], Loss: 0.0835\n",
            "Epoch [190/300], Loss: 0.1046\n",
            "Epoch [191/300], Loss: 0.1479\n",
            "Epoch [192/300], Loss: 0.1424\n",
            "Epoch [193/300], Loss: 0.0926\n",
            "Epoch [194/300], Loss: 0.1004\n",
            "Epoch [195/300], Loss: 0.1667\n",
            "Epoch [196/300], Loss: 0.0469\n",
            "Epoch [197/300], Loss: 0.0549\n",
            "Epoch [198/300], Loss: 0.1088\n",
            "Epoch [199/300], Loss: 0.1775\n",
            "Epoch [200/300], Loss: 0.1519\n",
            "Epoch [201/300], Loss: 0.0948\n",
            "Epoch [202/300], Loss: 0.1219\n",
            "Epoch [203/300], Loss: 0.1042\n",
            "Epoch [204/300], Loss: 0.1728\n",
            "Epoch [205/300], Loss: 0.1685\n",
            "Epoch [206/300], Loss: 0.1479\n",
            "Epoch [207/300], Loss: 0.3113\n",
            "Epoch [208/300], Loss: 0.2065\n",
            "Epoch [209/300], Loss: 0.1390\n",
            "Epoch [210/300], Loss: 0.1056\n",
            "Epoch [211/300], Loss: 0.3117\n",
            "Epoch [212/300], Loss: 0.1051\n",
            "Epoch [213/300], Loss: 0.0906\n",
            "Epoch [214/300], Loss: 0.1974\n",
            "Epoch [215/300], Loss: 0.2122\n",
            "Epoch [216/300], Loss: 0.4215\n",
            "Epoch [217/300], Loss: 0.1638\n",
            "Epoch [218/300], Loss: 0.0940\n",
            "Epoch [219/300], Loss: 0.0216\n",
            "Epoch [220/300], Loss: 0.2541\n",
            "Epoch [221/300], Loss: 0.1757\n",
            "Epoch [222/300], Loss: 0.1159\n",
            "Epoch [223/300], Loss: 0.1552\n",
            "Epoch [224/300], Loss: 0.0642\n",
            "Epoch [225/300], Loss: 0.2081\n",
            "Epoch [226/300], Loss: 0.1336\n",
            "Epoch [227/300], Loss: 0.0990\n",
            "Epoch [228/300], Loss: 0.1087\n",
            "Epoch [229/300], Loss: 0.1648\n",
            "Epoch [230/300], Loss: 0.1278\n",
            "Epoch [231/300], Loss: 0.0530\n",
            "Epoch [232/300], Loss: 0.2775\n",
            "Epoch [233/300], Loss: 0.1786\n",
            "Epoch [234/300], Loss: 0.0894\n",
            "Epoch [235/300], Loss: 0.1330\n",
            "Epoch [236/300], Loss: 0.0772\n",
            "Epoch [237/300], Loss: 0.1518\n",
            "Epoch [238/300], Loss: 0.0557\n",
            "Epoch [239/300], Loss: 0.1546\n",
            "Epoch [240/300], Loss: 0.1705\n",
            "Epoch [241/300], Loss: 0.1063\n",
            "Epoch [242/300], Loss: 0.0702\n",
            "Epoch [243/300], Loss: 0.1144\n",
            "Epoch [244/300], Loss: 0.0269\n",
            "Epoch [245/300], Loss: 0.3627\n",
            "Epoch [246/300], Loss: 0.0534\n",
            "Epoch [247/300], Loss: 0.0896\n",
            "Epoch [248/300], Loss: 0.1373\n",
            "Epoch [249/300], Loss: 0.0483\n",
            "Epoch [250/300], Loss: 0.3736\n",
            "Epoch [251/300], Loss: 0.1272\n",
            "Epoch [252/300], Loss: 0.0843\n",
            "Epoch [253/300], Loss: 0.2086\n",
            "Epoch [254/300], Loss: 0.1089\n",
            "Epoch [255/300], Loss: 0.2857\n",
            "Epoch [256/300], Loss: 0.0459\n",
            "Epoch [257/300], Loss: 0.0298\n",
            "Epoch [258/300], Loss: 0.0385\n",
            "Epoch [259/300], Loss: 0.0397\n",
            "Epoch [260/300], Loss: 0.0354\n",
            "Epoch [261/300], Loss: 0.0884\n",
            "Epoch [262/300], Loss: 0.1184\n",
            "Epoch [263/300], Loss: 0.1867\n",
            "Epoch [264/300], Loss: 0.1641\n",
            "Epoch [265/300], Loss: 0.0867\n",
            "Epoch [266/300], Loss: 0.0742\n",
            "Epoch [267/300], Loss: 0.0162\n",
            "Epoch [268/300], Loss: 0.1364\n",
            "Epoch [269/300], Loss: 0.1092\n",
            "Epoch [270/300], Loss: 0.0678\n",
            "Epoch [271/300], Loss: 0.0763\n",
            "Epoch [272/300], Loss: 0.0819\n",
            "Epoch [273/300], Loss: 0.1308\n",
            "Epoch [274/300], Loss: 0.0489\n",
            "Epoch [275/300], Loss: 0.1194\n",
            "Epoch [276/300], Loss: 0.0313\n",
            "Epoch [277/300], Loss: 0.1441\n",
            "Epoch [278/300], Loss: 0.0163\n",
            "Epoch [279/300], Loss: 0.1720\n",
            "Epoch [280/300], Loss: 0.0971\n",
            "Epoch [281/300], Loss: 0.0337\n",
            "Epoch [282/300], Loss: 0.1242\n",
            "Epoch [283/300], Loss: 0.0072\n",
            "Epoch [284/300], Loss: 0.3562\n",
            "Epoch [285/300], Loss: 0.0454\n",
            "Epoch [286/300], Loss: 0.0919\n",
            "Epoch [287/300], Loss: 0.3497\n",
            "Epoch [288/300], Loss: 0.0639\n",
            "Epoch [289/300], Loss: 0.1576\n",
            "Epoch [290/300], Loss: 0.1992\n",
            "Epoch [291/300], Loss: 0.1386\n",
            "Epoch [292/300], Loss: 0.0237\n",
            "Epoch [293/300], Loss: 0.1726\n",
            "Epoch [294/300], Loss: 0.0790\n",
            "Epoch [295/300], Loss: 0.0541\n",
            "Epoch [296/300], Loss: 0.1961\n",
            "Epoch [297/300], Loss: 0.0455\n",
            "Epoch [298/300], Loss: 0.0791\n",
            "Epoch [299/300], Loss: 0.0325\n",
            "Epoch [300/300], Loss: 0.0017\n"
          ]
        }
      ],
      "source": [
        "aer_dataset = Data()\n",
        "cnn = ConvNet(3, aer_dataset)\n",
        "cnn.to(device)\n",
        "cnn.train_model()\n",
        "torch.save(cnn, \"/model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn, \"/content/data/MyDrive/dl/model2.pth\")"
      ],
      "metadata": {
        "id": "JEZvqhOZbHFl"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWNPkZ04UCp"
      },
      "source": [
        "## START TEST\n",
        "\n",
        "\n",
        "Accuracy of the network: 70.1171875 %\n",
        "\n",
        "Accuracy of positive: 63.80597014925373 %\n",
        "\n",
        "Accuracy of neutral: 55.367231638418076 %\n",
        "\n",
        "Accuracy of negative: 77.54749568221071 %\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2c1Cu8n2Wv",
        "outputId": "cfd6c237-b2f5-466f-a7c2-fc86f4fddef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 66.6015625 %\n",
            "Accuracy of positive: 58.208955223880594 %\n",
            "Accuracy of neutral: 49.438202247191015 %\n",
            "Accuracy of negative: 75.77854671280276 %\n"
          ]
        }
      ],
      "source": [
        "test = TestConvNet(cnn, aer_dataset)\n",
        "n_class = test.test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display the confusion matrix as a heatmap\n",
        "arr = confusion_matrix(aer_dataset.test_y.detach().cpu().numpy(), n_class)\n",
        "class_names = ['Positive', 'Neutral', ' Negative']\n",
        "print(arr)\n",
        "df_cm = pd.DataFrame(arr, class_names, class_names)\n",
        "plt.figure(figsize = (9,6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "b3Ok6cs_J9K_",
        "outputId": "02087ca6-30aa-4c32-e1e9-b5d13c340662"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[159  26  86]\n",
            " [ 37 102  47]\n",
            " [125  64 402]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awUU1AREn3fk"
      },
      "source": [
        "## INFERENCE\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-sKsHj4ZwS",
        "outputId": "8f624e50-d86b-4e60-8e10-0cc5d79a1a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([9.8371655e-01, 6.2508939e-04, 1.5658503e-02], dtype=float32)]\n",
            "positive\n"
          ]
        }
      ],
      "source": [
        "inf_X = inference('/content/data/MyDrive/dl/angry2.wav')\n",
        "X = Norm(inf_X)\n",
        "y = cnn.forward(X)\n",
        "y = y.cpu().detach().numpy()\n",
        "predict = [np.exp(c) for c in y]\n",
        "max = np.argmax(predict)\n",
        "print(predict)\n",
        "print(classes[max])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "train_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}